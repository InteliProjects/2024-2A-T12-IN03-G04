{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Predict Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Introdu√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Este notebook criado pelo grupo Predict Codes cont√©m a an√°lise de dados da Rede Gazeta. O objetivo √© identificar padr√µes de receita, analisar o impacto de diferentes ve√≠culos, setores e segmentos para que, assim, seja poss√≠vel prever a receita futura.\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Agora, para a plena execu√ß√£o do notebook, √© necess√°rio primeiramente acessar [este link do drive](https://drive.google.com/drive/folders/1gPtxw2ZwFKJgWMN1R-kwkeerCG9_fNwz?usp=drive_link) com o email institucional do Inteli, baixar os arquivos necess√°rios e inserir eles na pasta \"base_de_dados\", dentro da pasta \"notebooks\". Ap√≥s isso, ser√° poss√≠vel executar o comando Run All."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Carregamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa, ser√£o instaladas e importadas as principais bibliotecas que ser√£o utilizadas em todo o notebook, bem como a importa√ß√£o das bases de dados utilizadas no modelo preditivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas matplotlib seaborn scikit-learn scipy lightgbm optuna ipywidgets pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das principais bibliotecas\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "# Manipula√ß√£o de dados e estat√≠sticas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Plotagem de gr√°ficos e visualiza√ß√£o de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Widgets interativos\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Machine Learning\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, \n",
    "    mean_absolute_percentage_error, \n",
    "    mean_squared_error, \n",
    "    r2_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Otimiza√ß√£o\n",
    "import optuna\n",
    "\n",
    "# Configura√ß√µes de exibi√ß√£o do Pandas e alertas\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bases de dados\n",
    "# Base de de dados de agosto (principal)\n",
    "df = pd.read_csv('base_de_dados/BaseDados_ProjetoINTELI_RG_01_AGOSTO_2024.xlsx - BASE.csv')\n",
    "# Base de de dados de ocup\n",
    "df_ocup = pd.read_csv('base_de_dados/BASE INTELI_META_OCUP.xlsx - OCUPA√á√ÉO .csv')\n",
    "# Base de dados de meta\n",
    "df_meta = pd.read_csv('base_de_dados/BASE INTELI_META_OCUP.xlsx - META.csv')\n",
    "# Base de dados de audi√™ncia\n",
    "df_audiencia = pd.read_csv('base_de_dados/AUDIENCIA_07_08_2024_COMPLETA.xlsx - AUDIENCIA.csv')\n",
    "# Base de dados econ√¥micos\n",
    "df_dados_economicos = pd.read_csv('base_de_dados/DadosEconomicos_ES_Inteli.xlsx - VarejoTotal.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Explora√ß√£o dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;A explora√ß√£o de dados pode ser compreendida como a etapa inicial para o tratamento dos dados, na qual os se examinam suas principais caracter√≠sticas, padr√µes e distribui√ß√µes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifica√ß√£o das colunas num√©ricas e categ√≥ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Ao realizar a explora√ß√£o de dados, √© necess√°rio compreender quais s√£o os tipos dos dados que comp√µem as bases de dados utilizadas. Para a realiza√ß√£o da classifica√ß√£o das colunas, foi feita uma an√°lise visual, na qual observamos e classificamos manualmente os dados presentes em cada coluna. Esse m√©todo foi necess√°rio devido a inconsist√™ncias identificadas na descri√ß√£o autom√°tica dos tipos de dados por meio do c√≥digo, que apresentava erros, que podem ser obvervados abaixo. Esses erros foram corrigidos posteriormente durante a etapa de limpeza dos dados, permitindo uma classifica√ß√£o mais precisa entre colunas num√©ricas e categ√≥ricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe os tipos de dados de cada coluna no df principal\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados de meta/ocupa√ß√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe os tipos de dados de cada coluna no df_meta\n",
    "df_meta.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe os tipos de dados de cada coluna no df_ocup\n",
    "df_ocup.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados de audi√™ncia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe os tipos de dados de cada coluna no df_audiencia\n",
    "df_audiencia.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estat√≠stica descritiva das colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de dados principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de dados econ√¥micos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dados_economicos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de dados de audi√™ncia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audiencia.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gr√°ficos para visualiza√ß√£o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo para implanta√ß√£o dos gr√°ficos\n",
    "corr_df = df.select_dtypes(include=[np.number])\n",
    "corr_matrix = corr_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Pr√©-Processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Etapa fundamental para evitar o surgimento de vieses nos dados. Dados que s√£o mal preparados ou n√£o passam por um bom pr√©-processamento podem diminuir a assertividade do modelo preditivo.\n",
    "Ao longo dessa se√ß√£o, as bases de dados foram limpadas, corrigidas e normalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Tratamento de missing values, identifica√ß√£o de outliers e corre√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Nessa etapa foram tratados os valores ausentes e duplicados, corrigido os tipos de colunas e identificado os outliers da base de dados principal, de meta/ocupa√ß√£o e de audi√™ncia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados principal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao longo desse bloco de c√≥digos, foram tratados os problemas existentes nessa base de dados, como missing values, colunas num√©ricas que est√£o com tipo \"object\", identifica√ß√£o de outliers, remo√ß√£o de colunas com valores ausentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica√ß√£o dos valores nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o das colunas num√©ricas que est√£o sendo tratadas como objetos\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remo√ß√£o de coluna com valores vazios\n",
    "df = df.dropna(axis=1, thresh=int(0.6 * len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corre√ß√£o das colunas num√©ricas\n",
    "colunas_numericas = ['VL Tabela', 'Desconto R$','Desc %', 'Vl Bruto', 'Vl Liquido Final', 'IPCA ES', 'IPCA BR', 'Taxa Ac. TRI % PIB']\n",
    "for coluna in colunas_numericas:\n",
    "  if df[coluna].dtype == 'object':\n",
    "    df[coluna] = df[coluna].str.replace('.', '')\n",
    "    df[coluna] = df[coluna].str.replace(',', '.')\n",
    "    df[coluna] = df[coluna].str.replace('%', '.')\n",
    "    df[coluna] = df[coluna].astype(float)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os valores nulos da coluna do PIB dizem respeito ao segundo trimestre de 2024\n",
    "# Como o resultado ainda n√£o foi divulgado, os valores nulos foram completados com estimativas do resultado do PIB\n",
    "df['Taxa Ac. TRI % PIB'].fillna(2.1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Setor'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o se ainda h√° valores faltantes\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_principais = ['VL Tabela', 'Vl Bruto', 'Vl Liquido Final']\n",
    "# Calcular o Z-Score para cada coluna num√©rica\n",
    "z_scores_df = stats.zscore(df[colunas_principais])\n",
    "\n",
    "# Criar um DataFrame com os Z-Scores\n",
    "z_scores_df = pd.DataFrame(z_scores_df, columns=colunas_principais)\n",
    "\n",
    "# Identificar os outliers (considerando Z-Score > 3 ou < -3 como outliers)\n",
    "outliers_df = (z_scores_df > 3) | (z_scores_df < -3)\n",
    "\n",
    "# Visualizar as linhas que possuem outliers em alguma das colunas\n",
    "outliers_df = df[outliers_df.any(axis=1)]\n",
    "\n",
    "outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar o menor outlier em cada uma das colunas\n",
    "menor_outlier_vl_tabela = outliers_df['VL Tabela'].min()\n",
    "menor_outlier_vl_bruto = outliers_df['Vl Bruto'].min()\n",
    "menor_outlier_vl_liquido_final = outliers_df['Vl Liquido Final'].min()\n",
    "\n",
    "# Exibir os menores outliers para cada coluna\n",
    "print(f\"Menor outlier em 'VL Tabela': {menor_outlier_vl_tabela}\")\n",
    "print(f\"Menor outlier em 'Vl Bruto': {menor_outlier_vl_bruto}\")\n",
    "print(f\"Menor outlier em 'Vl Liquido Final': {menor_outlier_vl_liquido_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica√ß√£o dos valores duplicados\n",
    "principal_duplicadas = df[df.duplicated()]\n",
    "principal_duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remo√ß√£o das linhas duplicadas\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados de meta/ocupa√ß√£o:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na base de meta/ocupa√ß√£o, verificou-se que n√£o existiam outliers, valores faltando e linhas duplicadas. No entanto, foi necess√°rio a adequa√ß√£o da coluna de m√™s ano e ocupa√ß√£o(%) para valores mais adequados para se trabalhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica√ß√£o dos valores nulos\n",
    "df_ocup.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o das colunas num√©ricas que est√£o sendo tratadas como objetos\n",
    "df_ocup.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando as datas em um formato de datetime\n",
    "meses = {\n",
    "    'jan.': '01',\n",
    "    'fev.': '02',\n",
    "    'mar.': '03',\n",
    "    'abr.': '04',\n",
    "    'mai.': '05',\n",
    "    'jun.': '06',\n",
    "    'jul.': '07',\n",
    "    'ago.': '08',\n",
    "    'set.': '09',\n",
    "    'out.': '10',\n",
    "    'nov.': '11',\n",
    "    'dez.': '12'\n",
    "}\n",
    "\n",
    "for mes_pt, mes_num in meses.items():\n",
    "    df_ocup['M√™s/ano'] = df_ocup['M√™s/ano'].str.replace(mes_pt, mes_num)\n",
    "\n",
    "df_ocup['M√™s/ano'] = pd.to_datetime(df_ocup['M√™s/ano'], format='%m-%y')\n",
    "\n",
    "# Transformando a coluna de 'Ocupa√ß√£o (%)' em num√©rica\n",
    "if df_ocup['Ocupa√ß√£o (%)'].dtype == 'object':\n",
    "    df_ocup['Ocupa√ß√£o (%)'] = df_ocup['Ocupa√ß√£o (%)'].str.replace('%', '')\n",
    "    df_ocup['Ocupa√ß√£o (%)'] = df_ocup['Ocupa√ß√£o (%)'].astype(float)\n",
    "df_ocup.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o Z-Score para a coluna de Ocupa√ß√£o (%)\n",
    "z_scores_df_ocup = stats.zscore(df_ocup['Ocupa√ß√£o (%)'])\n",
    "\n",
    "# Criar um DataFrame com os Z-Scores\n",
    "z_scores_df_ocup = pd.DataFrame(z_scores_df_ocup)\n",
    "\n",
    "# Exibir os primeiros valores dos Z-Scores\n",
    "z_scores_df_ocup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar os outliers (considerando Z-Score > 3 ou < -3 como outliers)\n",
    "outliers_df_ocup = (z_scores_df_ocup > 3) | (z_scores_df_ocup < -3)\n",
    "\n",
    "# Visualizar as linhas que possuem outliers na coluna de Ocupa√ß√£o (%)\n",
    "outliers_df_ocup = df_ocup[outliers_df_ocup.any(axis=1)]\n",
    "\n",
    "outliers_df_ocup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica√ß√£o dos valores duplicados\n",
    "ocup_duplicadas = df_ocup[df_ocup.duplicated()]\n",
    "ocup_duplicadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados de audi√™ncia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na limpeza da base de dados de audi√™ncia, notou-se que n√£o haviam valores duplicados e faltantes. Havia a presen√ßa apenas de um outlier, e foi necess√°rio transformar as colunas de Internet e \"Portal G1/GE/Home\" de string para num√©ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica√ß√£o dos valores nulos\n",
    "df_audiencia.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o das colunas num√©ricas que est√£o sendo tratadas como objetos\n",
    "df_audiencia.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando colunas em string para num√©ricas\n",
    "colunas_audiencia = ['GAZETA FM VIT√ìRIA', 'LITORAL FM', 'CBN VITORIA', \n",
    "                     'R√ÅDIO MIX VIT√ìRIA', 'INTERNET', 'PORTAL G1/GE/HOME']\n",
    "colunas_audiencia_string = ['INTERNET', 'PORTAL G1/GE/HOME']\n",
    "\n",
    "for coluna in colunas_audiencia_string:\n",
    "  if df_audiencia[coluna].dtype == 'object':\n",
    "    df_audiencia[coluna] = df_audiencia[coluna].str.replace('.', '')\n",
    "    df_audiencia[coluna] = df_audiencia[coluna].astype(float)\n",
    "df_audiencia.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o Z-Score para cada coluna num√©rica\n",
    "z_scores_df_audiencia = stats.zscore(df_audiencia[colunas_audiencia])\n",
    "\n",
    "# Criar um DataFrame com os Z-Scores\n",
    "z_scores_df_audiencia = pd.DataFrame(z_scores_df_audiencia, columns=colunas_audiencia)\n",
    "\n",
    "# Identificar os outliers (considerando Z-Score > 3 ou < -3 como outliers)\n",
    "outliers_df_audiencia = (z_scores_df_audiencia > 3) | (z_scores_df_audiencia < -3)\n",
    "\n",
    "# Visualizar as linhas que possuem outliers em alguma das colunas\n",
    "outliers_df_audiencia = df_audiencia[outliers_df_audiencia.any(axis=1)]\n",
    "\n",
    "outliers_df_audiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica√ß√£o dos valores duplicados\n",
    "duplicadas_audiencia = df_audiencia[df_audiencia.duplicated()]\n",
    "duplicadas_audiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria coluna de Vl Liquido Final sem normaliza√ß√£o para ser usada na modelo LightGBM\n",
    "df['Vl Liquido Final s/ normaliza√ß√£o'] = df['Vl Liquido Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Vl Liquido Final s/ norm'] = df['Vl Liquido Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veiculo_grafico = df.groupby('Veiculo')['Vl Liquido Final'].sum().reset_index()\n",
    "df_veiculo_grafico.sort_values(by='Vl Liquido Final', ascending=False, inplace=True)\n",
    "df_veiculo_principais = df_veiculo_grafico.head(2)\n",
    "df_veiculo_menores = df_veiculo_grafico.tail(17)\n",
    "df_veiculo_menores['Vl Liquido Final'].sum()\n",
    "\n",
    "grafico_pizza = [df_veiculo_menores['Vl Liquido Final'].sum(), df_veiculo_principais['Vl Liquido Final'].sum()]\n",
    "keys = ['Outros ve√≠culos', 'TV Gazeta e Internet']\n",
    "explode = (0.1, 0) \n",
    "# plotting data on chart \n",
    "plt.pie(grafico_pizza, labels=keys, explode=explode, shadow=True, startangle=90,\n",
    "        autopct='%.0f%%') \n",
    "  \n",
    "# displaying chart \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìè Normaliza√ß√£o das vari√°veis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;A normaliza√ß√£o de vari√°veis num√©ricas √© um processo realizado para a transforma√ß√£o dos dados para uma escala comum, feita para garantir que n√£o haja uma domin√¢ncia de algumas vari√°veis no modelo, devido sua magnitude. Portanto, realizou-se a an√°lise da distribui√ß√£o dos dados das colunas das nossas base de dados para assim compreender quais necessitavam de normaliza√ß√£o e depois essa foi aplicada como √© demonstrado abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;A base de dados possui as seguintes colunas num√©ricas: 'VL Tabela', 'Desconto R$', 'Desc %', 'Vl Bruto', 'Vl Liquido Final', 'IPCA ES', 'IPCA BR' e 'Taxa Ac. TRI % PIB'. A an√°lise descritiva revelou uma grande disparidade nos valores relacionados √† receita e ao desconto, o que justifica a aplica√ß√£o da transforma√ß√£o logar√≠tmica para normalizar essas colunas e tratar os outliers presentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise descritiva das colunas de receita\n",
    "df[['VL Tabela', 'Desconto R$', 'Vl Bruto', 'Vl Liquido Final']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Para aplicar essa normaliza√ß√£o, utilizamos a fun√ß√£o `np.log1p`, que calcula o logaritmo natural de (1 + valor) para cada dado. Sendo uma transforma√ß√£o, adequada para ajustar vari√°veis com ampla varia√ß√£o, pois reduz a diferen√ßa entre valores altos e baixos, estabilizando a vari√¢ncia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coluna \"VL Tabela\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Primeiramente, realizou-se a normaliza√ß√£o, por meio da Transforma√ß√£o Logar√≠tmica, da coluna \"VL Tabela\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a transforma√ß√£o logar√≠tmica √† coluna 'VL Tabela'\n",
    "df['VL Tabela'] = np.log1p(df['VL Tabela'])\n",
    "\n",
    "#Adiciona o m√©todo a uma vari√°vel para que ele possa ser utilizado na normaliza√ß√£o\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df['VL Tabela'] = scaler.fit_transform(df[['VL Tabela']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Ao aplicar a transforma√ß√£o logar√≠tmica, conseguimos reduzir a assimetria da distribui√ß√£o dos dados na coluna 'VL Tabela'. Isso pode ser observado no gr√°fico de distribui√ß√£o a seguir, onde a forma dos dados est√° mais centralizada e pr√≥xima de uma distribui√ß√£o normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_barras = \"lightsalmon\"\n",
    "cor_kde = \"black\"\n",
    "\n",
    "# Gr√°fico de Histograma\n",
    "sns.histplot(df['VL Tabela'], kde=False, stat=\"density\", color=cor_barras, label=\"Histograma de Frequ√™ncia\")\n",
    "\n",
    "# Adicionar o KDE separadamente com cor e label personalizados\n",
    "sns.kdeplot(df['VL Tabela'], color=cor_kde, label='Curva de Densidade Estimada (KDE)')\n",
    "\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"upper right\", fontsize=6)  # Adiciona a legenda no canto superior direito\n",
    "\n",
    "# Ajustando o t√≠tulo e os eixos\n",
    "plt.title('Gr√°fico de Distribui√ß√£o: VL Tabela\"', fontsize=12)\n",
    "plt.xlabel('                                     Valores de VL Tabela               ln(1+x)', fontsize=12)\n",
    "plt.ylabel('Densidade', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coluna \"Desconto R$\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Antes de realizar a normaliza√ß√£o, foi necess√°rio a transforma√ß√£o dos valores de descontos negativos em valores absolutos, para que assim o m√©todo de normaliza√ß√£o escolhido realizasse a normaliza√ß√£o da forma esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altera os valores da coluna de desconto para valores absolutos\n",
    "df[['Desconto R$']] = df[['Desconto R$']].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s a adequa√ß√£o dos valores, aplicou-se o mesmo m√©todo de normaliza√ß√£o da coluna anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a transforma√ß√£o logar√≠tmica √† coluna 'Desconto R$'\n",
    "df['Desconto R$'] = np.log1p(df['Desconto R$'])\n",
    "df['Desconto R$']= scaler.fit_transform(df[['Desconto R$']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Assim, percebe-se que transforma√ß√£o logar√≠tmica reduziu a assimetria na coluna 'Desconto R$', melhorando a distribui√ß√£o dos dados. Embora ainda haja concentra√ß√£o em torno de zero, a curva de densidade mostra uma distribui√ß√£o mais equilibrada, como √© poss√≠vel observar no gr√°fico abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de Distribui√ß√£o com a linha de densidade (KDE)\n",
    "sns.histplot(df['Desconto R$'], kde=False, stat=\"density\", color=cor_barras, label=\"Histograma de Frequ√™ncia\")\n",
    "sns.kdeplot(df['Desconto R$'], color=cor_kde, linewidth=1.3, label=\"Curva de Densidade Estimada (KDE)\")  # Limita a KDE para come√ßar no 0\n",
    "\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"upper right\")  # Adiciona a legenda no canto superior direito\n",
    "\n",
    "# Ajustando o t√≠tulo e os eixos\n",
    "plt.title('Gr√°fico de Distribui√ß√£o dos Valores da Coluna \"Desconto R$\"', fontsize=12)\n",
    "plt.xlabel('                                                       Valores da coluna \"Desconto R$\"                                       ln(1+x)', fontsize=12)\n",
    "plt.ylabel('Densidade', fontsize=12)\n",
    "\n",
    "# Remover legendas extras e linhas desnecess√°rias\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibindo o gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coluna \"Vl Bruto\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a transforma√ß√£o logar√≠tmica √† coluna \"Vl Bruto\"\n",
    "df['Vl Bruto'] = np.log1p(df['Vl Bruto'])\n",
    "df['Vl Bruto'] = scaler.fit_transform(df[['Vl Bruto']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Assim como nas colunas acima, a aplica√ß√£o da normaliza√ß√£o reduziu a dispers√£o dos dados, aproximando os valores de uma distribui√ß√£o normal, como √© poss√≠vel observar no gr√°fico abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de Histograma\n",
    "sns.histplot(df['Vl Bruto'], kde=False, stat=\"density\", color=cor_barras, label=\"Histograma de Frequ√™ncia\")\n",
    "\n",
    "# Adicionar o KDE separadamente com cor e label personalizados\n",
    "sns.kdeplot(df['Vl Bruto'], color=cor_kde, label='Curva de Densidade Estimada (KDE)')\n",
    "\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"upper right\", fontsize=6)  # Adiciona a legenda no canto superior direito\n",
    "\n",
    "# Ajustando o t√≠tulo e os eixos\n",
    "plt.title('Gr√°fico de Distribui√ß√£o: Vl Bruto\"', fontsize=12)\n",
    "plt.xlabel('                                                       Valores de Vl Bruto                                       ln(1+x)', fontsize=12)\n",
    "plt.ylabel('Densidade', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coluna \"Vl Liquido Final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a transforma√ß√£o logar√≠tmica √† coluna \"Vl Liquido Final\"\n",
    "df_sem_escala = df.copy()\n",
    "df['Vl Liquido Final'] = np.log1p(df['Vl Liquido Final'])\n",
    "df['Vl Liquido Final'] = scaler.fit_transform(df[['Vl Liquido Final']])\n",
    "df['Vl Liquido Final']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Por fim, a normaliza√ß√£o da coluna \"VL L√≠quido Final\" tamb√©m apresentou os resultados esperados, possibilitando uma redu√ß√£o na variabilidade dos dados e atenuando a influ√™ncia dos outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de Histograma\n",
    "sns.histplot(df['Vl Bruto'], kde=False, stat=\"density\", color=cor_barras, label=\"Histograma de Frequ√™ncia\")\n",
    "\n",
    "# Adicionar o KDE separadamente com cor e label personalizados\n",
    "sns.kdeplot(df['Vl Bruto'], color=cor_kde, label='Curva de Densidade Estimada (KDE)')\n",
    "\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"upper right\", fontsize=6)  # Adiciona a legenda no canto superior direito\n",
    "\n",
    "# Ajustando o t√≠tulo e os eixos\n",
    "plt.title('Gr√°fico de Distribui√ß√£o: Vl Liquido Final', fontsize=12)\n",
    "plt.xlabel('                              Valores de Vl Liquido Final           ln(1+x)', fontsize=12)\n",
    "plt.ylabel('Densidade', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados econ√¥micos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na base de dados Economicos , existem tr√™s colunas com vari√°veis num√©ricas que necessitavam ser normalizadas para que pudessem ser utilizadas em modelos preditivos e an√°lises estat√≠sticas. As colunas normalizadas foram as  'PMC - N√∫mero-√≠ndice (2022=100) (N√∫mero-√≠ndice)' , 'PMC - N√∫mero-√≠ndice com ajuste sazonal (2022=100) (N√∫mero-√≠ndice)'e 'PMC - Varia√ß√£o m√™s/m√™s imediatamente anterior, com ajuste sazonal (M/M-1) (%)'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplifica os nomes das colunas para que elas fiquem com uma visualiza√ß√£o mais simplificada e direta\n",
    "df_dados_economicos.rename(columns={'PMC - N√∫mero-√≠ndice (2022=100) (N√∫mero-√≠ndice)': 'pmc_indice',\n",
    "                                    'PMC - N√∫mero-√≠ndice com ajuste sazonal (2022=100) (N√∫mero-√≠ndice)': 'pmc_indice_sazonal',\n",
    "                                    'PMC - Varia√ß√£o m√™s/m√™s imediatamente anterior, com ajuste sazonal (M/M-1) (%)': 'pmc_variacao' }, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleta duas linhas da tabela onde continham dados faltantes\n",
    "df_dados_economicos = df_dados_economicos.drop(index=0)\n",
    "df_dados_economicos = df_dados_economicos.drop(index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforma as colunas as c√©lulas strings em num√©ricas\n",
    "col_num_economicos = ['pmc_indice', 'pmc_indice_sazonal', 'pmc_variacao']\n",
    "for coluna in col_num_economicos:\n",
    "  if df_dados_economicos[coluna].dtype == 'object':\n",
    "    df_dados_economicos[coluna] = df_dados_economicos[coluna].str.replace(',', '.')\n",
    "    df_dados_economicos[coluna] = df_dados_economicos[coluna].astype(float)\n",
    "df_dados_economicos.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolher o m√©todo Z-Score de Normaliza√ß√£o √© ideal para dados que seguem uma distribui√ß√£o aproximadamente normal (esse √© o caso das colunas dessa tabela), pois ele transforma os dados para uma escala com m√©dia 0 e desvio padr√£o 1. Isso √© especialmente √∫til para modelos estat√≠sticos que assumem normalidade e s√£o sens√≠veis √† escala dos dados, como regress√£o linear e modelos baseados em dist√¢ncia, garantindo consist√™ncia e comparabilidade entre as vari√°veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica o m√©todo \"Z-Score Normalization\" √†s colunas num√©ricas\n",
    "df_dados_economicos[col_num_economicos] = scaler.fit_transform(df_dados_economicos[col_num_economicos])\n",
    "\n",
    "df_dados_economicos.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coluna \"pmc_indice\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstra√ß√£o do Gr√°fico de Densidada da coluna \"pmc_indice'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_dados_economicos['pmc_indice'], kde=False, stat=\"density\", color=cor_barras, label=\"Histograma de Frequ√™ncia\")\n",
    "# Adicionar o KDE separadamente com cor e label personalizados\n",
    "sns.kdeplot(df_dados_economicos['pmc_indice'], color=cor_kde, label='Curva de Densidade Estimada (KDE)')\n",
    "plt.title('Gr√°fico de Distribui√ß√£o: pmc_indice', fontsize=12)\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"upper right\", fontsize=6)  # Adiciona a legenda no canto superior direito\n",
    "plt.xlabel('N√∫mero-√çndice do PMC')\n",
    "plt.ylabel('Densidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coluna \"mpc_indice_sazonal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstra√ß√£o do Gr√°fico de Densidada da coluna \"pmc_variacao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_dados_economicos['pmc_indice_sazonal'], kde=False, stat=\"density\", color=cor_barras, label=\"Histograma de Frequ√™ncia\")\n",
    "# Adicionar o KDE separadamente com cor e label personalizados\n",
    "sns.kdeplot(df_dados_economicos['pmc_indice_sazonal'], color=cor_kde, label='Curva de Densidade Estimada (KDE)')\n",
    "plt.title('Gr√°fico de Distribui√ß√£o: pmc_indice_sazonal', fontsize=12)\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"upper right\", fontsize=6)  # Adiciona a legenda no canto superior direito\n",
    "plt.xlabel('√çndice Sazonal do PMC')\n",
    "plt.ylabel('Densidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coluna \"pmc_variacao\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstra√ß√£o do Gr√°fico de Densidada da coluna \"pmc_variacao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_dados_economicos['pmc_variacao'], kde=False, stat=\"density\", color=cor_barras, label=\"Histograma de Frequ√™ncia\")\n",
    "# Adicionar o KDE separadamente com cor e label personalizados\n",
    "sns.kdeplot(df_dados_economicos['pmc_variacao'], color=cor_kde, label='Curva de Densidade Estimada (KDE)')\n",
    "plt.title('Gr√°fico de Distribui√ß√£o: pmc_variacao', fontsize=12)\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"upper right\", fontsize=6)  # Adiciona a legenda no canto superior direito\n",
    "plt.xlabel('Varia√ß√£o do PMC')\n",
    "plt.ylabel('Densidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora √© mostrado o come√ßinho da tabela para que seja poss√≠vel visualizar um peda√ßo do resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dados_economicos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  üî§ Codifica√ß√£o das vari√°veis categ√≥ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A aplica√ß√£o de vari√°veis categ√≥ricas em an√°lise de dados e machine learning envolve a convers√£o de dados qualitativos, como r√≥tulos ou categorias, em uma forma num√©rica que pode ser compreendida por algoritmos. Vari√°veis categ√≥ricas representam diferentes grupos ou n√≠veis, como \"M√™s\", \"Setor\" ou \"Origem\", e s√£o utilizados para titular de forma qualitativa certos atributos em um conjunto de dados. A correta codifica√ß√£o dessas vari√°veis √© indispens√°vel, pois permite que modelos preditivos e an√°lises estat√≠sticas utilizem informa√ß√µes categ√≥ricas de maneira eficaz, garantindo a integridade dos resultados e a melhoria da precis√£o dos modelos. Tamb√©m √© importante ressaltar que a codifica√ß√£o errada de vari√°veis pode acarretar em grave erro de c√°lculo nos modelos preditivos.\n",
    "\n",
    "Foi utilizado inicialmente o m√©todo de 'Label Encoder' para codificar todas as colunas que continham vari√°veis categ√≥ricas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona √† vari√°vel um m√©todo de Label Encoder, que ir√° aplicar o m√©todo Label √†s colunas\n",
    "le = LabelEncoder()\n",
    "\n",
    "#Faz com que o programa pare de realizar downcasting, pois isso ser√° impossibilitado de acontecer \\\n",
    "# nas pr√≥ximas vers√µes do pandas e estava deixando uma c√≥digo de exe√ß√£o ap√≥s algumas codifica√ß√µes\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados principal:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na base de dados principal, foram identificadas colunas com vari√°veis categ√≥ricas que necessitavam de codifica√ß√£o para que pudessem ser utilizadas em modelos preditivos e an√°lises estat√≠sticas. As colunas codificadas incluem \"UEN\", \"Veiculo\", \"Cliente\", \"Origem\", \"Segmento\", \"Setor\" e \"M√™s/ano\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica a coluna 'UEN'\n",
    "df['UEN'] = df['UEN'].fillna('')\n",
    "df['UEN_encoded'] = le.fit_transform(df['UEN'])\n",
    "df[['UEN','UEN_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codifica a coluna 'Ve√≠culo'\n",
    "df['Veiculo'] = df['Veiculo'].fillna('')\n",
    "df['Veiculo_encoded'] = le.fit_transform(df['Veiculo'])\n",
    "df[['Veiculo','Veiculo_encoded']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codifica a coluna 'Cliente'\n",
    "df['Cliente'] = df['Cliente'].fillna('')\n",
    "df['Cliente_encoded'] = le.fit_transform(df['Cliente'])\n",
    "df[['Cliente','Cliente_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codifica a coluna 'Origem'\n",
    "df['Origem'] = df['Origem'].fillna('')\n",
    "df['Origem_encoded'] = le.fit_transform(df['Origem'])\n",
    "df[['Origem','Origem_encoded']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codifica a coluna 'Segmento'\n",
    "df['Segmento'] = df['Segmento'].fillna('')\n",
    "df['Segmento_encoded'] = le.fit_transform(df['Segmento'])\n",
    "df[['Segmento','Segmento_encoded']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codifica a coluna 'Setor'\n",
    "df['Setor'] = df['Setor'].fillna('')\n",
    "df['Setor_encoded'] = le.fit_transform(df['Setor'])\n",
    "df[['Setor','Setor_encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A codifica√ß√£o da coluna 'M√™s/ano' √© um pouco diferente das outras pois exige alguns passos a mais para se adequar aos par√¢metros de codifica√ß√£o dos m√©todos utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento manual dos meses em portugu√™s, o m√©tedo de encoding s√≥ funciona corretamente quando os meses est√£o em ingl√™s\n",
    "meses = {'jan.': 'Jan', 'fev.': 'Feb', 'mar.': 'Mar', 'abr.': 'Apr',\n",
    "         'mai.': 'May', 'jun.': 'Jun', 'jul.': 'Jul', 'ago.': 'Aug',\n",
    "         'set.': 'Sep', 'out.': 'Oct', 'nov.': 'Nov', 'dez.': 'Dec'}\n",
    "\n",
    "# Muda os meses para ingl√™s\n",
    "df['M√™s/ano'] = df['M√™s/ano'].replace(meses, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converte os meses escritos para n√∫mero\n",
    "df['M√™s/ano'] = pd.to_datetime(df['M√™s/ano'], format='%b-%y', errors='coerce')\n",
    "\n",
    "# Formata para o formato esperado de apresenta√ß√£o\n",
    "df['M√™s/ano'] = df['M√™s/ano'].dt.strftime('%Y%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeaia o nome da coluna para se adequar a um padr√£o de escrita;\n",
    "df.rename(columns={'M√™s/ano': 'Ano/m√™s'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta parte, as colunas que passaram por codifica√ß√£o ser√£o removidas do dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluir_colunas = ['UEN', 'Veiculo', 'Cliente', 'Origem', 'Segmento', 'Setor', 'CONCATENAR']\n",
    "substituir_colunas = {\n",
    "    'UEN_encoded': 'UEN',\n",
    "    'Veiculo_encoded': 'Veiculo',\n",
    "    'Cliente_encoded': 'Cliente',\n",
    "    'Origem_encoded': 'Origem',\n",
    "    'Segmento_encoded': 'Segmento',\n",
    "    'Setor_encoded': 'Setor',\n",
    "}\n",
    "\n",
    "#Exclui as colunas que foram definidas na lista\n",
    "df = df.drop(columns=excluir_colunas)\n",
    "df = df.rename(columns=substituir_colunas)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assim ficaram as colunas e as codifica√ß√µes desse 'data frame'\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados de meta/ocupa√ß√£o:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta √© a p√°gina de **Meta** da base de dados, foram identificadas colunas com vari√°veis categ√≥ricas que necessitavam de codifica√ß√£o. As colunas codificadas incluem \"Uen\", \"M√™s\" e \"M√™s/ano\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codificando vari√°veis para que elas fiquem codificadas iguais a numera√ß√£o real dos meses do ano \n",
    "df_meta['M√™s'] = df_meta['M√™s'].fillna('')\n",
    "\n",
    "df_meta['mes_encoded'] = (df_meta['M√™s'].replace('Janeiro', 1)\n",
    "                                     .replace('Fevereiro', 2)\n",
    "                                     .replace('Mar√ßo', 3)\n",
    "                                     .replace('Abril', 4)\n",
    "                                     .replace('Maio', 5)\n",
    "                                     .replace('Junho', 6)\n",
    "                                     .replace('Julho', 7)\n",
    "                                     .replace('Agosto', 8)\n",
    "                                     .replace('Setembro', 9)\n",
    "                                     .replace('Outubro', 10)\n",
    "                                     .replace('Novembro', 11)\n",
    "                                     .replace('Dezembro', 12))\n",
    "\n",
    "df_view = df_meta['M√™s'].drop_duplicates()\n",
    "df_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substitui os valores vazios por strings vazias\n",
    "df_meta['Uen'] = df_meta['Uen'].fillna('')\n",
    "\n",
    "#Define exatamente quais vari√°veis ser√£o trocadas por quais n√∫meros\n",
    "df_meta['Uen_encoded'] = (df_meta['Uen'].replace('Digital',0)\n",
    "                                              .replace('R√°dio',1)\n",
    "                                              .replace('Televis√£o',2))\n",
    "\n",
    "#mostra a equival√™ncia entre a coluna normal e a codificada\n",
    "df_view2 = df_meta[['Uen_encoded','Uen']].drop_duplicates()\n",
    "df_view2.sort_values(by='Uen_encoded', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muda o nome da coluna para padroniza√ß√£o\n",
    "df_meta.rename(columns={'M√™s/ano': 'Ano/m√™s'}, inplace=True)\n",
    "\n",
    "#muda os nomes dos meses para ingl√™s\n",
    "df_meta['Ano/m√™s'] = df_meta['Ano/m√™s'].replace(meses, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convers√£o dos meses escritos para n√∫mero\n",
    "df_meta['Ano/m√™s'] = pd.to_datetime(df_meta['Ano/m√™s'], format='%b-%y', errors='coerce')\n",
    "\n",
    "# Formata para o formato de apresenta√ß√£o do n√∫mero esperado\n",
    "df_meta['Ano/m√™s'] = df_meta['Ano/m√™s'].dt.strftime('%Y%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionando as colunas que ser√£o exclu√≠das da tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substituir_colunas = {\n",
    "    'Uen_encoded': 'UEN',\n",
    "    'mes_encoded': 'Mes'\n",
    "}\n",
    "\n",
    "#lista de colunas que ser√£o exclu√≠das\n",
    "meta_excluir_colunas = ['M√™s', 'Uen']\n",
    "\n",
    "#retira as colunas n√£o codificadas\n",
    "df_meta = df_meta.drop(columns=meta_excluir_colunas)\n",
    "\n",
    "df_meta = df_meta.rename(columns=substituir_colunas)\n",
    "\n",
    "#Primeiras 5 linhas do resultado final:\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta √© a parte de **Ocupa√ß√£o** da base de dados. As colunas codificadas incluem \"Uen\", \"M√™s\" e \"M√™s/ano\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocup['Ve√≠culo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codificando vari√°veis para que elas sejam atribu√≠das a um n√∫mero inteiro para o modelo conseguir interpretar elas.\n",
    "df_ocup['Ve√≠culo'] = df_ocup['Ve√≠culo'].fillna('')\n",
    "\n",
    "df_ocup['Ve√≠culo_encoded'] = (df_ocup['Ve√≠culo'].replace('CBN VITORIA', 0)\n",
    "                                     .replace('GAZETA FM LINHARES', 1)\n",
    "                                     .replace('GAZETA FM VITORIA', 2)\n",
    "                                     .replace('LITORAL FM', 5)\n",
    "                                     .replace('LITORAL FM NOROESTE', 6)\n",
    "                                     .replace('LITORAL FM NORTE', 7)\n",
    "                                     .replace('LITORAL FM SUL', 8)\n",
    "                                     .replace('R√ÅDIO MIX VIT√ìRIA', 11)\n",
    "                                     .replace('TV GAZETA', 12)\n",
    "                                     .replace('TV GAZETA NOROESTE', 13)\n",
    "                                     .replace('TV GAZETA NORTE', 14)\n",
    "                                     .replace('TV GAZETA SUL', 15))\n",
    "df_ocup_unique = df_ocup[['Ve√≠culo', 'Ve√≠culo_encoded']].drop_duplicates(subset=['Ve√≠culo', 'Ve√≠culo_encoded'])\n",
    "\n",
    "# Exibe o DataFrame resultante sem duplicatas\n",
    "df_ocup_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocup = df_ocup.drop('Ve√≠culo', axis=1)\n",
    "substituir_colunas = {'Ve√≠culo_encoded': 'Veiculo'}\n",
    "df_ocup = df_ocup.rename(columns=substituir_colunas)\n",
    "df_ocup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados econ√¥micos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na base de dados referentes aos Dados Econ√¥micos, foi identificada a coluna de \"M√™s\" que necessita de codifica√ß√£o. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muda os meses para ingl√™s\n",
    "df_dados_economicos['M√™s'] = df_dados_economicos['M√™s'].replace(meses, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converte os meses escritos para n√∫mero\n",
    "df_dados_economicos['M√™s'] = pd.to_datetime(df_dados_economicos['M√™s'], format='%b-%y', errors='coerce')\n",
    "\n",
    "# Formata para o formato esperado de apresenta√ß√£o\n",
    "df_dados_economicos['M√™s'] = df_dados_economicos['M√™s'].dt.strftime('%Y%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeaia o nome da coluna para se adequar a um padr√£o de escrita;\n",
    "df_dados_economicos.rename(columns={'M√™s': 'Ano/m√™s'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primeiras 5 linhas do resultado final:\n",
    "df_dados_economicos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base de dados de audi√™ncia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na base de dados de audi√™ncia, existem alguns processos que precisam ser realizados antes da codifica√ß√£o de vari√°veis, para que cada registro dessa tabela relacione per√≠odo, ve√≠culo e audi√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformula o formato da tabela, de modo que siga o mesmo formato das outras tabelas tratadas\n",
    "df_audiencia = pd.melt(df_audiencia, id_vars=['PERIODO'], var_name='VEICULO', value_name='AUDIENCIA')\n",
    "\n",
    "df_audiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o registro dos per√≠odos para formata√ß√£o em ingl√™s\n",
    "df_audiencia['PERIODO'] = df_audiencia['PERIODO'].replace(meses, regex=True)\n",
    "\n",
    "df_audiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converte os meses escritos para n√∫mero\n",
    "df_audiencia['PERIODO'] = pd.to_datetime(df_audiencia['PERIODO'], format='%b-%y', errors='coerce')\n",
    "\n",
    "df_audiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formata para o formato esperado de apresenta√ß√£o\n",
    "df_audiencia['PERIODO'] = df_audiencia['PERIODO'].dt.strftime('%Y%m')\n",
    "\n",
    "df_audiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formata o tipo de dados da vari√°vel 'PERIODO' para n√∫mero\n",
    "df_audiencia['PERIODO'] = df_audiencia['PERIODO'].astype(int)\n",
    "\n",
    "df_audiencia.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica os ve√≠culos da tabela\n",
    "df_audiencia['VEICULO'] = df_audiencia['VEICULO'].fillna('')\n",
    "df_audiencia['VEICULO_encoded'] = le.fit_transform(df_audiencia['VEICULO'])\n",
    "df_audiencia[['VEICULO','VEICULO_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mant√©m apenas os valores codificados\n",
    "df_audiencia['VEICULO'] = df_audiencia['VEICULO_encoded']\n",
    "df_audiencia.drop('VEICULO_encoded', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Concatena√ß√£o de tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz com que todas as colunas de cada tabela sejam mostrados\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define duas vari√°veis diferentes que ir√£o guardar apenas os dados da regi√µes espec√≠ficas, sendo Brasil ou Espirto Santo \n",
    "df_pmc_es = df_dados_economicos[df_dados_economicos['Brasil e Unidade da Federa√ß√£o'] == 'Esp√≠rito Santo']\n",
    "df_pmc_br = df_dados_economicos[df_dados_economicos['Brasil e Unidade da Federa√ß√£o'] == 'Brasil']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muda o nome das colunas para que elas se adequem a nova organiza√ß√£o -> regi√£o Brasil\n",
    "df_pmc_br = df_pmc_br.rename(columns={'pmc_indice': 'pmc_indice_Brasil', 'pmc_indice_sazonal': 'pmc_indice_sazonal_Brasil', 'pmc_variacao': 'pmc_variacao_Brasil'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muda o nome das colunas para que elas se adequem a nova organiza√ß√£o -> regi√£o Espirito Santo\n",
    "df_pmc_es = df_pmc_es.rename(columns={'pmc_indice': 'pmc_indice_ES', 'pmc_indice_sazonal': 'pmc_indice_sazonal_ES', 'pmc_variacao': 'pmc_variacao_ES'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adiciona as colunas que cont√©m pmc na tabela principal seguindo a distin√ß√£o de Ano/m√™s:\n",
    "df_combinedES = pd.merge(df, df_pmc_es, on='Ano/m√™s', how='inner', validate='many_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combina ambas as tabelas com as colunas atualizadas em uma nova tabela atualizada:\n",
    "df_combined_total = pd.merge(df_combinedES , df_pmc_br, on='Ano/m√™s', how='inner', validate='many_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclui as colunas duplicadas ou que n√£o ser√£o mais utilizadas na nova tabela\n",
    "df = df_combined_total.drop(['Brasil e Unidade da Federa√ß√£o_y', 'Brasil e Unidade da Federa√ß√£o_x', 'Tipos de √≠ndice_y', 'Tipos de √≠ndice_x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostra as um pouco de como ficou a nova tabela com suas colunas:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Explora√ß√£o dos dados p√≥s realiza√ß√£o de pr√©-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Estat√≠stica Descritiva "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Nessa etapa, ap√≥s a realiza√ß√£o do pr√©-processamento, transforma√ß√£o e normaliza√ß√£o das vari√°veis num√©ricas, √© poss√≠vel visualizar gr√°ficos e visualizar as estat√≠sticas descritivas das colunas de modo mais assertivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise descritiva da base de dados principal\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise descritiva da base de dados de ocupa√ß√£o\n",
    "df_ocup.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise descritiva da base de dados de meta\n",
    "df_meta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise descritiva da base de dados de audi√™ncia\n",
    "df_audiencia.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise descritiva da base de dados ecom√¥micos\n",
    "df_dados_economicos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Gr√°ficos para Visualiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adcionado\n",
    "df_hist = df.drop(['Ano', 'M√™s'], axis=1)\n",
    "df_hist.hist(bins=30, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "numericas_colunas = ['Ano', 'M√™s', 'Ano/m√™s', 'Vl Liquido Final', 'Vl Bruto', 'VL Tabela', 'Desconto R$', 'Desc %', 'IPCA ES', 'IPCA BR', 'Taxa Ac. TRI % PIB', 'pmc_indice_ES', 'pmc_indice_sazonal_ES', 'pmc_variacao_ES', 'pmc_indice_Brasil','pmc_indice_sazonal_Brasil', 'pmc_variacao_Brasil']\n",
    "corr_df = df[numericas_colunas]\n",
    "corr_matrix = corr_df.corr()\n",
    "\n",
    "\n",
    "# Ajusta o tamanho do gr√°fico e a formata√ß√£o das anota√ß√µes\n",
    "plt.figure(figsize=(15, 8))  # Ajusta o tamanho da figura\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".3f\", cmap='coolwarm', annot_kws={\"size\": 10})  # fmt=\".4f\" para limitar a 4 casas decimais \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veiculo_grafico = df.groupby('Veiculo')['Vl Liquido Final'].sum().reset_index()\n",
    "df_veiculo_grafico.sort_values(by='Vl Liquido Final', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Hip√≥teses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;As hip√≥teses s√£o suposi√ß√µes criadas sobre os dados, com base na an√°lise explorat√≥ria realizada inicialmente. Elas representam poss√≠veis rela√ß√µes, padr√µes ou comportamentos que esperamos encontrar nos dados ap√≥s o pr√©-processamento. Apresenta-se abaixo as hip√≥teses desenvolvidas a partir das observa√ß√µes iniciais, seguidas dos gr√°ficos e an√°lises que visam comprovar sua veracidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hip√≥tese 1 - Correla√ß√£o entre Condi√ß√µes Econ√¥micas e Faturamento: Vari√°veis macroecon√¥micas, como a infla√ß√£o ou PIB regional, est√£o correlacionadas com o faturamento dos ve√≠culos de comunica√ß√£o, refletindo a capacidade de investimento dos anunciantes, afetando diretamente a organiza√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "numericas_colunas = ['Ano', 'M√™s', 'Ano/m√™s', 'Vl Liquido Final', 'Vl Bruto', 'VL Tabela', 'Desconto R$', 'Desc %', 'IPCA ES', 'IPCA BR', 'Taxa Ac. TRI % PIB', 'pmc_indice_ES', 'pmc_indice_sazonal_ES', 'pmc_variacao_ES', 'pmc_indice_Brasil','pmc_indice_sazonal_Brasil', 'pmc_variacao_Brasil']\n",
    "corr_df = df[numericas_colunas]\n",
    "corr_matrix = corr_df.corr()\n",
    "\n",
    "\n",
    "# Ajusta o tamanho do gr√°fico e a formata√ß√£o das anota√ß√µes\n",
    "plt.figure(figsize=(15, 8))  # Ajusta o tamanho da figura\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".3f\", cmap='coolwarm', annot_kws={\"size\": 10})  # fmt=\".4f\" para limitar a 4 casas decimais \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hip√≥tese 2 - Sazonalidade Impacta o Faturamento: O faturamento da Rede Gazeta varia significativamente conforme as esta√ß√µes do ano, meses ou datas comemorativas. Por exemplo, o faturamento tende a aumentar durante o per√≠odo de festas de fim de ano devido ao maior investimento em publicidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df[['VL Tabela', 'Vl Bruto', 'Vl Liquido Final', 'M√™s']].groupby('M√™s').mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(x='M√™s', y='VL Tabela', data=df_grouped, marker='o', label='VL Tabela')\n",
    "sns.lineplot(x='M√™s', y='Vl Bruto', data=df_grouped, marker='o', label='Vl Bruto')\n",
    "sns.lineplot(x='M√™s', y='Vl Liquido Final', data=df_grouped, marker='o', label='Vl Liquido Final')\n",
    "\n",
    "plt.title('Sazonalidade por M√™s')\n",
    "plt.ylabel('Valores')\n",
    "plt.xlabel('M√™s')\n",
    "plt.legend(title='Legenda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hip√≥tese 3 - Origem Geogr√°fica da Venda e o Faturamento: A origem geogr√°fica da venda (cidade espec√≠fica do Esp√≠rito Santo ou mercado nacional) influencia significativamente o faturamento da Rede Gazeta. Vendas provenientes do mercado nacional geram maior faturamento em compara√ß√£o com vendas realizadas em cidades do Esp√≠rito Santo, devido ao maior alcance e potencial de investimento de clientes de fora do estado. Dessa forma, pode-se prever a propor√ß√£o entre clientes do mercado nacional e regional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_columns = ['VL Tabela', 'Vl Bruto', 'Desc %', 'Vl Liquido Final']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 12), sharex=True)\n",
    "\n",
    "for i, barplot_column in enumerate(barplot_columns):\n",
    "    row = i // 2\n",
    "    col = i % 2  \n",
    "    sns.barplot(x='Origem', y=barplot_column, data=df, estimator=sum, ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'Soma de {barplot_column} por Origem')\n",
    "    axes[row, col].set_xlabel('Origem')\n",
    "    axes[row, col].set_ylabel(f'Soma de {barplot_column}')\n",
    "    axes[row, col].tick_params(axis='x', rotation=70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Gr√°ficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Nesta se√ß√£o ser√£o apresentados e documentados todos os gr√°ficos produzidos a partir da an√°lise explorat√≥ria dos dados, os quais s√£o essenciais para entendimento visual acerca dos dados fornecidos, bem como para confirma√ß√£o ou nega√ß√£o das hip√≥teses levantadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base de dados principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Aqui ser√£o disponibilizados os gr√°ficos que se relacionam, isto √©, que foram formulados a partir das an√°lises descritivas feitas sobre a base de dados principal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagrama de caixa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Um diagrama de caixa √© uma representa√ß√£o gr√°fica que resume a distribui√ß√£o de um conjunto de dados atrav√©s de cinco principais estat√≠sticas: m√≠nimo, primeiro quartil (Q1), mediana, terceiro quartil (Q3) e m√°ximo. Ele visualiza a dispers√£o e a assimetria dos dados, destacando a presen√ßa de *outliers* e a concentra√ß√£o dos dados em torno da mediana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de caixa personalizado\n",
    "df_bp = df_hist.drop('Desc %', axis=1)\n",
    "colunas_categoricas = ['Ano/m√™s', 'Veiculo', 'Cliente', 'Origem', 'Segmento', 'Setor', 'UEN']\n",
    "colunas_financeiras = ['VL Tabela', 'Desconto R$', 'Vl Bruto', 'Vl Liquido Final']\n",
    "colunas_indicadores_econ = ['IPCA ES', 'IPCA BR', 'Taxa Ac. TRI % PIB', 'pmc_indice_ES', 'pmc_indice_sazonal_ES', 'pmc_variacao_ES', 'pmc_indice_Brasil', 'pmc_indice_sazonal_Brasil', 'pmc_variacao_Brasil']\n",
    "df_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(data=df_bp[colunas_indicadores_econ])\n",
    "plt.title('Distribui√ß√£o dos Indicadores Econ√¥micos')\n",
    "plt.ylabel('Valores dos Indicadores (%)')\n",
    "plt.xlabel('Indicadores Econ√¥micos')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(data=df_bp[colunas_financeiras])\n",
    "plt.title('Distribui√ß√£o das Colunas Financeiras')\n",
    "plt.ylabel('Valores das Colunas Financeiras (em base logari√≠timica)')\n",
    "plt.xlabel('Colunas Financeiras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(data=df_bp['Vl Liquido Final s/ normaliza√ß√£o'])\n",
    "plt.title('Distribui√ß√£o dos Valores L√≠quidos Finais de cada contrato')\n",
    "plt.ylabel('Valores L√≠quidos (em milh√µes)')\n",
    "plt.xlabel('Valor L√≠quido Final sem normaliza√ß√£o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Um histograma √© uma representa√ß√£o gr√°fica que mostra a distribui√ß√£o de um conjunto de dados num√©ricos ao dividir os dados em intervalos e contar a frequ√™ncia de valores dentro de cada intervalo. No gr√°fico, os intervalos s√£o representados por barras, cuja altura indica o n√∫mero de ocorr√™ncias em cada intervalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas\n",
    "df_hist.hist(bins=30, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de histograma\n",
    "columns_sct = ['VL Tabela', 'Vl Bruto', 'Vl Liquido Final']\n",
    "\n",
    "n = len(columns_sct)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n, figsize=(5 * n, 6), sharex=True, sharey=True)\n",
    "\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, column_sct in enumerate(columns_sct):\n",
    "    sns.histplot(df_hist[column_sct], kde=True, stat=\"density\", ax=axes[i])\n",
    "    axes[i].set_title(f'Histograma de {column_sct}')\n",
    "    axes[i].set_xlabel(f'{column_sct}')\n",
    "    axes[i].set_ylabel('Densidade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gr√°fico de linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Um gr√°fico de linhas √© uma representa√ß√£o visual que conecta pontos de dados individuais ao longo de uma linha cont√≠nua, geralmente usada para mostrar a evolu√ß√£o de uma vari√°vel ao longo do tempo. No eixo horizontal (x), s√£o plotados os valores de uma vari√°vel independente, enquanto no eixo vertical (y), s√£o exibidos os valores da vari√°vel dependente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot\n",
    "df_grouped = df[['VL Tabela', 'Vl Bruto', 'Vl Liquido Final', 'M√™s']].groupby('M√™s').mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(x='M√™s', y='VL Tabela', data=df_grouped, marker='o', label='VL Tabela')\n",
    "sns.lineplot(x='M√™s', y='Vl Bruto', data=df_grouped, marker='o', label='Vl Bruto')\n",
    "sns.lineplot(x='M√™s', y='Vl Liquido Final', data=df_grouped, marker='o', label='Vl Liquido Final')\n",
    "\n",
    "plt.title('Sazonalidade por M√™s')\n",
    "plt.ylabel('Valores')\n",
    "plt.xlabel('M√™s')\n",
    "plt.legend(title='Legenda')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gr√°ficos de barras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Um gr√°fico de barras √© uma representa√ß√£o visual que utiliza barras retangulares para comparar valores entre diferentes categorias ou grupos. No gr√°fico, o comprimento de cada barra √© proporcional ao valor que ela representa. O eixo horizontal exibe as categorias, enquanto o outro eixo mostra as frequ√™ncias ou valores associados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°ficos de barras\n",
    "barplot_columns = ['VL Tabela', 'Vl Bruto', 'Desc %', 'Vl Liquido Final']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 12), sharex=True)\n",
    "\n",
    "for i, barplot_column in enumerate(barplot_columns):\n",
    "    row = i // 2\n",
    "    col = i % 2  \n",
    "    sns.barplot(x='Origem', y=barplot_column, data=df, estimator=sum, ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'Soma de {barplot_column} por Origem')\n",
    "    axes[row, col].set_xlabel('Origem')\n",
    "    axes[row, col].set_ylabel(f'Soma de {barplot_column}')\n",
    "    axes[row, col].tick_params(axis='x', rotation=70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de correla√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Uma matriz de correla√ß√£o √© uma tabela que mostra os coeficientes de correla√ß√£o entre todas as poss√≠veis combina√ß√µes de vari√°veis em um conjunto de dados. Cada c√©lula na matriz exibe um valor que indica a for√ßa e a dire√ß√£o da rela√ß√£o linear entre duas vari√°veis, variando de -1 (correla√ß√£o negativa perfeita) a 1 (correla√ß√£o positiva perfeita), com 0 indicando aus√™ncia de correla√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "numericas_colunas = ['Ano', 'M√™s', 'Ano/m√™s', 'Vl Liquido Final', 'Vl Bruto', 'VL Tabela', 'Desconto R$', 'Desc %', 'IPCA ES', 'IPCA BR', 'Taxa Ac. TRI % PIB', 'pmc_indice_ES', 'pmc_indice_sazonal_ES', 'pmc_variacao_ES', 'pmc_indice_Brasil','pmc_indice_sazonal_Brasil', 'pmc_variacao_Brasil']\n",
    "corr_df = df[numericas_colunas]\n",
    "corr_matrix = corr_df.corr()\n",
    "\n",
    "\n",
    "# Ajusta o tamanho do gr√°fico e a formata√ß√£o das anota√ß√µes\n",
    "plt.figure(figsize=(15, 8))  # Ajusta o tamanho da figura\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".4f\", cmap='coolwarm', annot_kws={\"size\": 10})  # fmt=\".4f\" para limitar a 4 casas decimais \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Modelos Avaliados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### √Årvore de Decis√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Random Forest √© uma modelo supervisionado baseada em √°rvores de decis√£o que combina diversas √°rvores para melhorar a precis√£o das previs√µes e reduzir o overfitting. Ao longo desse bloco de c√≥digo, foi feita a escolha de features, separa√ß√£o do dataframe em conjuntos de treino, teste e valida√ß√£o e, por fim, √© exibido as m√©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['M√™s', 'Veiculo', 'Origem', 'Segmento', 'Setor']] #df apenas com as features desejadas\n",
    "y = df['Vl Liquido Final'] #Coluna desejada para a previs√£o\n",
    "\n",
    "# Separando os dados em treino (60%), valida√ß√£o (20%) e teste (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Dividindo o conjunto tempor√°rio (X_temp) em valida√ß√£o e teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o modelo Random Forest com hiperpar√¢metros padr√µes\n",
    "rf_model = RandomForestRegressor(n_estimators=200, random_state=42, max_depth=30, min_samples_leaf=2, min_samples_split=10, max_features='sqrt')\n",
    "\n",
    "# Treinando o modelo com os dados de treino (somente colunas categ√≥ricas)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previs√µes no conjunto de valida√ß√£o\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Avaliando o desempenho no conjunto de valida√ß√£o\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)*100\n",
    "\n",
    "# Exibindo os resultados do conjunto de valida√ß√£o\n",
    "print(f\"Valida√ß√£o - R¬≤: {val_r2:.4f}\")\n",
    "print(f\"Valida√ß√£o - RMSE: {val_rmse:.2f}\")\n",
    "print(f\"Valida√ß√£o - MAE: {val_mae:.2f}\")\n",
    "print(f\"Valida√ß√£o - MAPE: {val_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rf_model.predict(X_test)\n",
    "# Avaliando o modelo no conjunto de teste\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Exibindo os resultados do conjunto de teste\n",
    "print(f\"Teste - R¬≤: {test_r2:.4f}\")\n",
    "print(f\"Teste - RMSE: {test_rmse:.2f}\")\n",
    "print(f\"Teste - MAE: {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regress√£o Linear M√∫ltipla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No modelo de regress√£o linear m√∫ltipla, foi necess√°rio criar um novo dataframe, agrupando as colunas com base no ve√≠culo, ano e m√™s. Ap√≥s isso, foi aplicado a normaliza√ß√£o em uma coluna, selecionado as features e feita a separa√ß√£o do dataframe em conjuntos de treino, teste e valida√ß√£o. Ap√≥s o treinamento do modelo, foram calculadas e exibidas as m√©tricas. Por fim, h√° a presen√ßa de gr√°ficos para facilitar a visualiza√ß√£o dos resultados do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrupa dados na tentativa de gerar um modelo mais preciso na predi√ß√£o\n",
    "df_veiculoR = df.groupby(['Veiculo', 'Ano', 'M√™s']).agg({\n",
    "    'Vl Liquido Final': 'sum', \n",
    "    'IPCA ES': 'mean',                  \n",
    "    'IPCA BR': 'mean',                 \n",
    "    'Taxa Ac. TRI % PIB': 'mean',    \n",
    "    'Vl Liquido Final s/ norm': 'sum', \n",
    "    'UEN': 'first',                    \n",
    "    'pmc_indice_ES': 'mean',            \n",
    "    'pmc_indice_sazonal_ES': 'mean',    \n",
    "    'pmc_variacao_ES': 'mean',         \n",
    "    'pmc_indice_Brasil': 'mean',       \n",
    "    'pmc_indice_sazonal_Brasil': 'mean',\n",
    "    'pmc_variacao_Brasil': 'mean'       \n",
    "}).reset_index()\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "df_veiculoR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veiculoR['Vl Liquido Final normalizado'] = np.log1p(df_veiculoR['Vl Liquido Final s/ norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "xR = df_veiculoR[['Ano', 'M√™s', 'Veiculo', 'Taxa Ac. TRI % PIB', 'pmc_indice_sazonal_ES', 'UEN']] #df apenas com as features desejadas\n",
    "yR = df_veiculoR['Vl Liquido Final normalizado'] #Coluna desejada para a previs√£o\n",
    "\n",
    "# Separando os dados em treino (60%), valida√ß√£o (20%) e teste (20%)\n",
    "X_trainR, X_tempR, y_trainR, y_tempR = train_test_split(xR, yR, test_size=0.4, random_state=42)\n",
    "\n",
    "# Dividindo o conjunto tempor√°rio (X_temp) em valida√ß√£o e teste\n",
    "X_valR, X_testR, y_valR, y_testR = train_test_split(X_tempR, y_tempR, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relaciona a fun√ß√£o da  regress√£o linear M√∫ltipla com uma vari√°vel que ser√° aplicada ao projeto\n",
    "modelR = LinearRegression()\n",
    "\n",
    "#Treina o modelo com os dados de treino\n",
    "modelR.fit(X_trainR, y_trainR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previs√µes com o conjunto de teste, ja utilizando o algoritmo que acabou de ser treinado\n",
    "y_predR = modelR.predict(X_testR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avalia a performance do modelo\n",
    "\n",
    "# Calcula o MSE mede a m√©dia das diferen√ßas absolutas entre os valores reais e previstos. \n",
    "# Ele indica, em m√©dia, o quanto os valores previstos se desviam dos valores reais.-> quanto menor o valor presvisto, melhor √© a qualidade da previs√£o \n",
    "mseR = mean_squared_error(y_testR, y_predR)\n",
    "print(f'MSE: {mseR}')\n",
    "\n",
    "# Calcula o R^2 √© uma m√©trica que indica o qu√£o bem o modelo explica a variabilidade dos dados. Ele varia de 0 a 1 -> quanto mais pr√≥ximo de 1 o resultado \n",
    "# estiver, melhor √© o modelo para os dados de treinamento\n",
    "r2R = r2_score(y_testR, y_predR)\n",
    "print(f'R^2: {r2R}')\n",
    "\n",
    "#Calcula o MAPE, expressa o erro m√©dio absoluto como uma porcentagem dos valores reais, \n",
    "# o que facilita a interpreta√ß√£o, pois mostra o erro em termos percentuais. -> quanto menor a porcentagem, melhor\n",
    "mapeR = np.mean(np.abs((y_testR - y_predR) / y_testR)) * 100\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mapeR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra os coeficientes das vari√°veis independentes\n",
    "print(f'Coeficientes: {modelR.coef_}')\n",
    "\n",
    "# Mosta a Intercepta√ß√£o\n",
    "print(f'Intercepta√ß√£o: {modelR.intercept_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra um gr√°fico do valor real contra o ideal do modelo\n",
    "plt.scatter(y_testR, y_predR, color='blue', label='Valores Previstos')\n",
    "plt.plot([y_testR.min(), y_testR.max()], [y_testR.min(), y_testR.max()], color='red', lw=1, label='Linha de Refer√™ncia/Modelo Ideal', linestyle='dashed')\n",
    "plt.xlabel('Valores Reais (log(1+x))')\n",
    "plt.ylabel('Previs√µes (log(1+x))')\n",
    "plt.title('Valores Reais vs Previs√µes Estat√≠sticas')\n",
    "plt.legend()  # Mostra a legenda\n",
    "plt.grid(linewidth = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibi os coeficietes -> esses coeficientes mostram o qu√£o diretamente relacionados cada t√≥pico est√° para o resultado final da predi√ß√£o\n",
    "coef_dfR = pd.DataFrame(modelR.coef_, xR.columns, columns=['Coeficientes'])\n",
    "\n",
    "print(coef_dfR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coluna = 'Veiculo'\n",
    "indice = 0\n",
    "\n",
    "veiculos_nomes = {\n",
    "    0: 'CBN VITORIA',\n",
    "    1: 'GAZETA FM LINHARES',\n",
    "    2: 'GAZETA FM VITORIA',\n",
    "    3: 'GAZETA PRODUCOES',\n",
    "    4: 'INTERNET',\n",
    "    5: 'LITORAL FM',\n",
    "    6: 'LITORAL FM NOROESTE',\n",
    "    7: 'LITORAL FM NORTE',\n",
    "    8: 'LITORAL FM SUL',\n",
    "    9: 'PORTAL G1/GE/HOME',\n",
    "    10: 'PRODUCAO',\n",
    "    11: 'RADIO MIX VITORIA',\n",
    "    12: 'TV GAZETA',\n",
    "    13: 'TV GAZETA NOROESTE',\n",
    "    14: 'TV GAZETA N0RTE',\n",
    "    15: 'TV GAZETA SUL',\n",
    "    16: 'TV NOROESTE PRODUCOES',\n",
    "    17: 'TV NORTE PRODUCOES',\n",
    "    18: 'TV SUL PRODUCOES'\n",
    "}\n",
    "\n",
    "origens_nomes = {\n",
    "    0: 'CH - CONTATO - CACHOEIRO',\n",
    "    1: 'CO - CONTATO - COLATINA',\n",
    "    2: 'LI - CONTATO - LINHARES',\n",
    "    3: 'MP - M√çDIA PROGRAM√ÅTICA',\n",
    "    4: 'RN - MERCADO NACIONAL',\n",
    "    5: 'VT - CONTATO - VIT√ìRIA'\n",
    "}\n",
    "\n",
    "segmentos_nomes = {\n",
    "    0: 'AGROPECUARIA',\n",
    "    1: 'ALIMENTOS',\n",
    "    2: 'BEBIDAS',\n",
    "    3: 'BENS INDUSTRIAIS MAT. PRIMA',\n",
    "    4: 'BRINQUEDOS E DIVERSOES',\n",
    "    5: 'COMERCIO',\n",
    "    6: 'CONDOMINIOS',\n",
    "    7: 'EDUCACAO/MEIOS DE COMUNICACAO',\n",
    "    8: 'EQUIP/MAT/ESCRIT/LOJA/ESCOLA',\n",
    "    9: 'FOTO / OTICA / CINE / SOM',\n",
    "    10: 'INDUSTRIA DA CONSTRUCAO',\n",
    "    11: 'INTERNET',\n",
    "    12: 'LIMPEZA / HIGIENE DOMESTICA',\n",
    "    13: 'MEIOS DE TRANSPORTES E AFINS',\n",
    "    14: 'MERCADO FINANCEIRO',\n",
    "    15: 'MOVEIS E DECORACOES',\n",
    "    16: 'OUTRAS RECEITAS',\n",
    "    17: 'OUTROS',\n",
    "    18: 'PERFUMARIA / FARMACIA',\n",
    "    19: 'SERVICOS',\n",
    "    20: 'SERVICOS DE TELECOMUNICACOES',\n",
    "    21: 'TEXTIL / VESTUARIO',\n",
    "    22: 'UTILIDADES DOMESTICAS'\n",
    "}\n",
    "\n",
    "dropdown1 = widgets.Dropdown(\n",
    "    options=['Veiculo', 'Origem', 'Segmento'],\n",
    "    value=None,\n",
    "    description='Prever por:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "dropdown2 = widgets.Dropdown(\n",
    "    options=[],\n",
    "    value = None,\n",
    "    description='√çndice:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "def atualizar_dropdown2(opcao):\n",
    "    global coluna\n",
    "    coluna = opcao['new']\n",
    "    \n",
    "    if coluna == 'Veiculo':\n",
    "        dropdown2.options = [(v, k) for k, v in veiculos_nomes.items()]\n",
    "    elif coluna == 'Origem':\n",
    "        dropdown2.options = [(v, k) for k, v in origens_nomes.items()]\n",
    "    elif coluna == 'Segmento':\n",
    "        dropdown2.options = [(v, k) for k, v in segmentos_nomes.items()]\n",
    "    \n",
    "    dropdown2.value = None\n",
    "    \n",
    "def on_value_change_dropdown2(opcao):\n",
    "    global indice\n",
    "    indice = opcao['new']\n",
    "\n",
    "dropdown1.observe(atualizar_dropdown2, names='value')\n",
    "dropdown2.observe(on_value_change_dropdown2, names='value')\n",
    "\n",
    "display(dropdown1)\n",
    "display(dropdown2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Selecionado: {coluna} ({indice})\\nPor favor, execute as c√©lulas abaixos novamentes para o resultado atualizado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coluna == 'Veiculo' or coluna == 'Origem' or coluna == 'Segmento':\n",
    "    df_temp = df.sort_values(by=coluna)\n",
    "    df_coluna = df_temp.groupby(coluna)\n",
    "\n",
    "dfs_novos = []\n",
    "for item_unico in df_temp[coluna].unique():\n",
    "    dfs_novos.append(df_coluna.get_group(item_unico))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coluna == 'Veiculo':\n",
    "    indice_max = df['Veiculo'].nunique()-1\n",
    "elif coluna == 'Origem':\n",
    "    indice_max = df['Origem'].nunique()-1\n",
    "elif coluna == 'Segmento':\n",
    "    indice_max = df['Segmento'].nunique()-1\n",
    "\n",
    "def escolher_df (df_escolhido):\n",
    "    if df_escolhido >= 0 or df_escolhido <= indice_max:\n",
    "        return dfs_novos[df_escolhido]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'√çndice m√°ximo ve√≠culo:', df['Veiculo'].nunique()-1)\n",
    "print(f'√çndice m√°ximo origem:', df['Origem'].nunique()-1)\n",
    "print(f'√çndice m√°ximo segmento:', df['Segmento'].nunique()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarima = escolher_df(indice)\n",
    "df_sarima = df_sarima[['Ano/m√™s', 'Vl Liquido Final']]\n",
    "df_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarima['Vl Liquido Final'] = df_sem_escala['Vl Liquido Final']\n",
    "df_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarima['Ano/m√™s'] = pd.to_datetime(df_sarima['Ano/m√™s'], format='%Y%m')\n",
    "df_sarima.set_index('Ano/m√™s', inplace=True)\n",
    "df_sarima = df_sarima.sort_index()\n",
    "\n",
    "df_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarima = df_sarima.groupby(df_sarima.index).sum()\n",
    "df_sarima = np.log1p(df_sarima)\n",
    "df_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarima = df_sarima.asfreq('MS')\n",
    "df_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarima = df_sarima.fillna(method='ffill')\n",
    "df_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = range(0, 3)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 6) for x in pdq]\n",
    "\n",
    "results = []\n",
    "\n",
    "y = df_sarima\n",
    "\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            model = SARIMAX(y, order=param, seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            result = model.fit()\n",
    "            results.append((param, param_seasonal, result.aic))\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ajustar ARIMA{param}x{param_seasonal}12: {e}')\n",
    "            continue\n",
    "\n",
    "if results:\n",
    "    best_params = sorted(results, key=lambda x: x[2])[0]\n",
    "    print(f'Melhor combina√ß√£o de par√¢metros: {best_params[0]}x{best_params[1]}12 - AIC: {best_params[2]}')\n",
    "else:\n",
    "    print(\"Nenhuma combina√ß√£o de par√¢metros conseguiu ajustar o modelo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df_sarima) * 0.8)\n",
    "train, test = df_sarima['Vl Liquido Final'][:train_size], df_sarima['Vl Liquido Final'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SARIMAX(train,\n",
    "                order=best_params[0],\n",
    "                seasonal_order=best_params[1],\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_model = model.fit(disp=False)\n",
    "\n",
    "sarima_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sarima_model.get_forecast(steps=len(test))\n",
    "forecasted_values = predictions.predicted_mean\n",
    "pred_ci = predictions.conf_int()\n",
    "\n",
    "ax = test.plot(label='Real', figsize=(10, 6))\n",
    "predictions.predicted_mean.plot(ax=ax, label='Previs√µes', alpha=0.7)\n",
    "ax.fill_between(pred_ci.index, pred_ci.iloc[:, 0], pred_ci.iloc[:, 1], color='k', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(test, forecasted_values)*100\n",
    "mae = mean_absolute_error(test, forecasted_values)\n",
    "mae_real = np.expm1(mae)\n",
    "mape = mean_absolute_percentage_error(test, forecasted_values)*100\n",
    "\n",
    "print(f'R¬≤ do modelo: {r2:.2f}%')\n",
    "print(f\"MAE do modelo: {mae}\")\n",
    "print(f'MAE real: {mae_real}')\n",
    "print(f\"MAPE do modelo: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Modelo Final - LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por conta do excelente desempenho na capacidade de previs√£o de receita e por ser capaz de criar previs√µes de diferentes combina√ß√µes, como por ve√≠culo e segmento e por ve√≠culo e origem, optamos por utilizar o modelo LightGBM como nosso modelo final. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 1 - Previs√£o por Ve√≠culo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_veiculo = df.groupby(['Veiculo', 'Ano', 'M√™s']).agg({\n",
    "    'IPCA ES': 'mean',                  \n",
    "    'IPCA BR': 'mean',                 \n",
    "    'Taxa Ac. TRI % PIB': 'mean',    \n",
    "    'Vl Liquido Final s/ normaliza√ß√£o': 'sum', \n",
    "    'UEN': 'first',                    \n",
    "    'pmc_indice_ES': 'mean',            \n",
    "    'pmc_indice_sazonal_ES': 'mean',    \n",
    "    'pmc_variacao_ES': 'mean',         \n",
    "    'pmc_indice_Brasil': 'mean',       \n",
    "    'pmc_indice_sazonal_Brasil': 'mean',\n",
    "    'pmc_variacao_Brasil': 'mean'       \n",
    "}).reset_index()\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "df_veiculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza√ß√£o da Coluna Vl Liquido ap√≥s somar os valores\n",
    "df_veiculo['Vl Liquido Final normalizado'] = np.log1p(df_veiculo['Vl Liquido Final s/ normaliza√ß√£o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_veiculo = df_veiculo[['Veiculo','Ano','M√™s', 'Taxa Ac. TRI % PIB']] #df apenas com as features desejadas\n",
    "y_veiculo = df_veiculo['Vl Liquido Final normalizado'] #Coluna alvo\n",
    "\n",
    "# Separando os dados em treino (60%), valida√ß√£o (20%) e teste (20%)\n",
    "X_train_veiculo, X_temp_veiculo, y_train_veiculo, y_temp_veiculo = train_test_split(x_veiculo, y_veiculo, test_size=0.4, random_state=42)\n",
    "\n",
    "# Dividindo o conjunto tempor√°rio (X_temp) em valida√ß√£o e teste\n",
    "X_val_veiculo, X_test_veiculo, y_val_veiculo, y_test_veiculo = train_test_split(X_temp_veiculo, y_temp_veiculo, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_veiculo = lgb.Dataset(X_train_veiculo, label=y_train_veiculo)\n",
    "val_data_veiculo = lgb.Dataset(X_val_veiculo, label=y_val_veiculo)\n",
    "\n",
    "params_veiculo = {'learning_rate': 0.06, \n",
    "          'num_leaves': 126, \n",
    "          'feature_fraction': 0.7, \n",
    "          'bagging_fraction': 0.9, \n",
    "          'bagging_freq': 10, \n",
    "          'n_estimators': 790,\n",
    "          'seed': 42}\n",
    "\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=100)]\n",
    "model_veiculo = lgb.train(params_veiculo, \n",
    "                  train_data_veiculo, \n",
    "                  num_boost_round=1000, \n",
    "                  valid_sets=[val_data_veiculo], \n",
    "                  callbacks=callbacks)\n",
    "\n",
    "\n",
    "y_pred_veiculo = model_veiculo.predict(X_test_veiculo, num_iteration=model_veiculo.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo das m√©tricas para o modelo\n",
    "rmse_veiculo = mean_squared_error(y_test_veiculo, y_pred_veiculo, squared=False)\n",
    "r2_veiculo = r2_score(y_test_veiculo, y_pred_veiculo)\n",
    "mae_veiculo = mean_absolute_error(y_test_veiculo, y_pred_veiculo)\n",
    "mape_veiculo = mean_absolute_percentage_error(y_test_veiculo, y_pred_veiculo)\n",
    "\n",
    "# Exibi√ß√£o das m√©tricas para o modelo\n",
    "print(f'Teste - RMSE: {rmse_veiculo:.4f}')\n",
    "print(f'Teste - R¬≤: {r2_veiculo:.4f}')\n",
    "print(f'Teste - MAE: {mae_veiculo:.4f}')\n",
    "print(f'Teste - MAPE: {mape_veiculo:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo 1 com otimiza√ß√£o dos hiperpar√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o n√≠vel de log do Optuna como WARNING para evitar impress√µes detalhadas\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Ignorar avisos\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Definir a fun√ß√£o objetivo para a otimiza√ß√£o\n",
    "def objective_veiculo_opt(trial):\n",
    "    params_veiculo_opt = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.07),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 130),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 0.8),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.7, 1),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1000),\n",
    "        'verbose': -1,  # Desativar logs desnecess√°rios\n",
    "        'seed': 42  \n",
    "    }\n",
    "    \n",
    "    train_data_veiculo_opt = lgb.Dataset(X_train_veiculo, label=y_train_veiculo)\n",
    "    val_data_veiculo_opt = lgb.Dataset(X_val_veiculo, label=y_val_veiculo)\n",
    "    \n",
    "    model_veiculo_opt = lgb.train(params_veiculo_opt, \n",
    "                                  train_data_veiculo_opt, \n",
    "                                  num_boost_round=1000, \n",
    "                                  valid_sets=[val_data_veiculo_opt], \n",
    "                                  callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "    \n",
    "    y_pred_veiculo_opt = model_veiculo_opt.predict(X_val_veiculo, num_iteration=model_veiculo_opt.best_iteration)\n",
    "    \n",
    "    mape_veiculo_opt = mean_absolute_percentage_error(y_val_veiculo, y_pred_veiculo_opt) * 100\n",
    "    return mape_veiculo_opt\n",
    "\n",
    "# Otimiza√ß√£o\n",
    "study_veiculo_opt = optuna.create_study(direction='minimize')  \n",
    "study_veiculo_opt.optimize(objective_veiculo_opt, n_trials=50)\n",
    "\n",
    "# Mostrar melhores hiperpar√¢metros\n",
    "print(\"Melhores hiperpar√¢metros encontrados para Ve√≠culo (Otimizado):\", study_veiculo_opt.best_params)\n",
    "\n",
    "# Treinar o modelo com os melhores hiperpar√¢metros\n",
    "best_params_veiculo_opt = study_veiculo_opt.best_params\n",
    "best_params_veiculo_opt['verbose'] = -1  # Desativar logs desnecess√°rios\n",
    "\n",
    "train_data_veiculo_opt = lgb.Dataset(X_train_veiculo, label=y_train_veiculo)\n",
    "val_data_veiculo_opt = lgb.Dataset(X_val_veiculo, label=y_val_veiculo)\n",
    "\n",
    "best_model_veiculo_opt = lgb.train(best_params_veiculo_opt, \n",
    "                                   train_data_veiculo_opt, \n",
    "                                   num_boost_round=1000, \n",
    "                                   valid_sets=[val_data_veiculo_opt], \n",
    "                                   callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "\n",
    "# Avalia√ß√£o final no conjunto de teste\n",
    "y_pred_test_veiculo_opt = best_model_veiculo_opt.predict(X_test_veiculo, num_iteration=best_model_veiculo_opt.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo das m√©tricas para o modelo otimizado\n",
    "rmse_veiculo_opt = mean_squared_error(y_test_veiculo, y_pred_test_veiculo_opt, squared=False)\n",
    "r2_veiculo_opt = r2_score(y_test_veiculo, y_pred_test_veiculo_opt)\n",
    "mae_veiculo_opt = mean_absolute_error(y_test_veiculo, y_pred_test_veiculo_opt)\n",
    "mape_veiculo_opt = mean_absolute_percentage_error(y_test_veiculo, y_pred_test_veiculo_opt)\n",
    "\n",
    "# Exibi√ß√£o das m√©tricas para o modelo otimizado\n",
    "print('M√©tricas com a Otimiza√ß√£o dos Hiperpar√¢metros')\n",
    "print(f'Teste - RMSE: {rmse_veiculo_opt:.4f}')\n",
    "print(f'Teste - R¬≤: {r2_veiculo_opt:.4f}')\n",
    "print(f'Teste - MAE: {mae_veiculo_opt:.4f}')\n",
    "print(f'Teste - MAPE: {mape_veiculo_opt:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar valores reais vs previstos para o modelo otimizado\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Garantir que os dados de teste estejam alinhados e sem reset de √≠ndices desnecess√°rios\n",
    "plt.plot(y_test_veiculo.reset_index(drop=True)[:100], label='Valores Reais', color='#6baed6', linestyle='-')\n",
    "plt.plot(y_pred_test_veiculo_opt[:100], label='Valores Previstos', color='red', linestyle='--')\n",
    "\n",
    "# Configura√ß√µes do gr√°fico\n",
    "plt.xlabel('√çndice')\n",
    "plt.ylabel('Valor ( ln(1+x) )')\n",
    "plt.title('An√°lise das Predi√ß√µes do Modelo 1: Valores Reais vs. Valores Previstos (Primeiros 100 Dados)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do gr√°fico\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Gr√°fico de dispers√£o dos valores reais vs previstos\n",
    "plt.scatter(y_test_veiculo, y_pred_test_veiculo_opt, alpha=0.6, edgecolors='k', color='blue', label='Valores reais em compara√ß√£o com os preditos')\n",
    "\n",
    "# Adicionar uma linha de refer√™ncia (linha ideal onde os valores reais s√£o iguais aos previstos)\n",
    "plt.plot([y_test_veiculo.min(), y_test_veiculo.max()], [y_test_veiculo.min(), y_test_veiculo.max()], 'r--', lw=2, label='Linha de Posi√ß√£o Ideal')\n",
    "\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"lower right\", fontsize = 10)  # Adiciona a legenda no canto inferior direito\n",
    "\n",
    "# R√≥tulos e t√≠tulo\n",
    "plt.xlabel('Valores Reais ( ln(1+x) )')\n",
    "plt.ylabel('Valores Preditos ( ln(1+x) )')\n",
    "plt.title('Diagrama de Dispers√£o das Predi√ß√µes do Modelo 1 (Ve√≠culo)')\n",
    "\n",
    "# Adicionar grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Exibir gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 2 - Previs√£o por Ve√≠culo e Segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segmento = df.groupby(['Veiculo', 'Ano', 'M√™s', 'Segmento']).agg({\n",
    "    'IPCA ES': 'mean',                  \n",
    "    'IPCA BR': 'mean',                 \n",
    "    'Taxa Ac. TRI % PIB': 'mean',    \n",
    "    'Vl Liquido Final s/ normaliza√ß√£o': 'sum', \n",
    "    'UEN': 'first',                    \n",
    "    'pmc_indice_ES': 'mean',            \n",
    "    'pmc_indice_sazonal_ES': 'mean',    \n",
    "    'pmc_variacao_ES': 'mean',         \n",
    "    'pmc_indice_Brasil': 'mean',       \n",
    "    'pmc_indice_sazonal_Brasil': 'mean',\n",
    "    'pmc_variacao_Brasil': 'mean'       \n",
    "}).reset_index()\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "df_segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segmento['Vl Liquido Final normalizado'] = np.log1p(df_segmento['Vl Liquido Final s/ normaliza√ß√£o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_segmento = df_segmento[['Veiculo','Ano','M√™s', 'Taxa Ac. TRI % PIB', 'Segmento']] #df apenas com as features desejadas\n",
    "y_segmento = df_segmento['Vl Liquido Final normalizado'] #Coluna desejada para a previs√£o\n",
    "\n",
    "# Separando os dados em treino (60%), valida√ß√£o (20%) e teste (20%)\n",
    "X_train_segmento, X_temp_segmento, y_train_segmento, y_temp_segmento = train_test_split(x_segmento, y_segmento, test_size=0.4, random_state=42)\n",
    "\n",
    "# Dividindo o conjunto tempor√°rio (X_temp) em valida√ß√£o e teste\n",
    "X_val_segmento, X_test_segmento, y_val_segmento, y_test_segmento = train_test_split(X_temp_segmento, y_temp_segmento, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_segmento = lgb.Dataset(X_train_segmento, label=y_train_segmento)\n",
    "val_data_segmento = lgb.Dataset(X_val_segmento, label=y_val_segmento)\n",
    "\n",
    "params_segmento = {'learning_rate': 0.06, \n",
    "          'num_leaves': 126, \n",
    "          'feature_fraction': 0.7, \n",
    "          'bagging_fraction': 0.9, \n",
    "          'bagging_freq': 10, \n",
    "          'n_estimators': 790,\n",
    "          'seed': 42}\n",
    "\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=100)]\n",
    "model_segmento = lgb.train(params_segmento, \n",
    "                  train_data_segmento, \n",
    "                  num_boost_round=1000, \n",
    "                  valid_sets=[val_data_segmento], \n",
    "                  callbacks=callbacks)\n",
    "\n",
    "\n",
    "y_pred_segmento = model_segmento.predict(X_test_segmento, num_iteration=model_segmento.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo das m√©tricas para o modelo\n",
    "rmse_segmento = mean_squared_error(y_test_segmento, y_pred_segmento, squared=False)\n",
    "r2_segmento = r2_score(y_test_segmento, y_pred_segmento)\n",
    "mae_segmento = mean_absolute_error(y_test_segmento, y_pred_segmento)\n",
    "mape_segmento = mean_absolute_percentage_error(y_test_segmento, y_pred_segmento)\n",
    "\n",
    "# Exibi√ß√£o das m√©tricas para o modelo\n",
    "print(f'Teste - RMSE: {rmse_segmento:.4f}')\n",
    "print(f'Teste - R¬≤: {r2_segmento:.4f}')\n",
    "print(f'Teste - MAE: {mae_segmento:.4f}')\n",
    "print(f'Teste - MAPE: {mape_segmento:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo 2 com otimiza√ß√£o dos hiperpar√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignorar avisos\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# definir a fun√ß√£o objetivo para a otimiza√ß√£o\n",
    "def objective_segmento_opt(trial):\n",
    "    params_segmento_opt = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.07),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 130),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 0.8),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.7, 1),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1000),\n",
    "        'verbose': -1,  # Desativar logs desnecess√°rios\n",
    "        'seed': 42  \n",
    "    }\n",
    "    \n",
    "    train_data_segmento_opt = lgb.Dataset(X_train_segmento, label=y_train_segmento)\n",
    "    val_data_segmento_opt = lgb.Dataset(X_val_segmento, label=y_val_segmento)\n",
    "    \n",
    "    model_segmento_opt = lgb.train(params_segmento_opt, \n",
    "                                   train_data_segmento_opt, \n",
    "                                   num_boost_round=1000, \n",
    "                                   valid_sets=[val_data_segmento_opt], \n",
    "                                   callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "    \n",
    "    y_pred_segmento_opt = model_segmento_opt.predict(X_val_segmento, num_iteration=model_segmento_opt.best_iteration)\n",
    "    \n",
    "    mape_segmento_opt = mean_absolute_percentage_error(y_val_segmento, y_pred_segmento_opt) * 100\n",
    "    return mape_segmento_opt\n",
    "\n",
    "# Otimiza√ß√£o\n",
    "study_segmento_opt = optuna.create_study(direction='minimize')  \n",
    "study_segmento_opt.optimize(objective_segmento_opt, n_trials=50)\n",
    "\n",
    "# Mostrar melhores hiperpar√¢metros\n",
    "print(\"Melhores hiperpar√¢metros encontrados para Segmento (Otimizado):\", study_segmento_opt.best_params)\n",
    "\n",
    "# Treinar o modelo com os melhores hiperpar√¢metros\n",
    "best_params_segmento_opt = study_segmento_opt.best_params\n",
    "best_params_segmento_opt['verbose'] = -1  # Desativar logs desnecess√°rios\n",
    "\n",
    "train_data_segmento_opt = lgb.Dataset(X_train_segmento, label=y_train_segmento)\n",
    "val_data_segmento_opt = lgb.Dataset(X_val_segmento, label=y_val_segmento)\n",
    "\n",
    "best_model_segmento_opt = lgb.train(best_params_segmento_opt, \n",
    "                                    train_data_segmento_opt, \n",
    "                                    num_boost_round=1000, \n",
    "                                    valid_sets=[val_data_segmento_opt], \n",
    "                                    callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "\n",
    "# Avalia√ß√£o final no conjunto de teste\n",
    "y_pred_test_segmento_opt = best_model_segmento_opt.predict(X_test_segmento, num_iteration=best_model_segmento_opt.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo das m√©tricas para o modelo otimizado\n",
    "rmse_segmento_opt = mean_squared_error(y_test_segmento, y_pred_test_segmento_opt, squared=False)\n",
    "r2_segmento_opt = r2_score(y_test_segmento, y_pred_test_segmento_opt)\n",
    "mae_segmento_opt = mean_absolute_error(y_test_segmento, y_pred_test_segmento_opt)\n",
    "mape_segmento_opt = mean_absolute_percentage_error(y_test_segmento, y_pred_test_segmento_opt)\n",
    "\n",
    "# Exibi√ß√£o das m√©tricas para o modelo otimizado\n",
    "print('M√©tricas com a Otimiza√ß√£o dos Hiperpar√¢metros')\n",
    "print(f'Teste - RMSE: {rmse_segmento_opt:.4f}')\n",
    "print(f'Teste - R¬≤: {r2_segmento_opt:.4f}')\n",
    "print(f'Teste - MAE: {mae_segmento_opt:.4f}')\n",
    "print(f'Teste - MAPE: {mape_segmento_opt:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar valores reais vs previstos para o modelo otimizado\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Garantir que os dados de teste estejam alinhados e sem reset de √≠ndices desnecess√°rios\n",
    "plt.plot(y_test_segmento.reset_index(drop=True)[:100], label='Valores Reais', color='#6baed6', linestyle='-')\n",
    "plt.plot(y_pred_test_segmento_opt[:100], label='Valores Previstos', color='red', linestyle='--')\n",
    "\n",
    "# Configura√ß√µes do gr√°fico\n",
    "plt.xlabel('√çndice')\n",
    "plt.ylabel('Valor ( ln(1+x) )')\n",
    "plt.title('An√°lise das Predi√ß√µes do Modelo 2: Valores Reais vs. Valores Previstos (Primeiros 100 Dados)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do gr√°fico\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Gr√°fico de dispers√£o dos valores reais vs previstos\n",
    "plt.scatter(y_test_segmento, y_pred_test_segmento_opt, alpha=0.6, edgecolors='k', color='blue', label='Valores reais em compara√ß√£o com os preditos')\n",
    "\n",
    "# Adicionar uma linha de refer√™ncia (linha ideal onde os valores reais s√£o iguais aos previstos)\n",
    "plt.plot([y_test_segmento.min(), y_test_segmento.max()], [y_test_segmento.min(), y_test_segmento.max()], 'r--', lw=2, label='Linha de Posi√ß√£o Ideal')\n",
    "\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"lower right\", fontsize = 10)  # Adiciona a legenda no canto inferior direito\n",
    "\n",
    "# R√≥tulos e t√≠tulo\n",
    "plt.xlabel('Valores Reais ( ln(1+x) )')\n",
    "plt.ylabel('Valores Preditos ( ln(1+x) )')\n",
    "plt.title('Diagrama de Dispers√£o das Predi√ß√µes do Modelo 2 (Ve√≠culo e Segmento)')\n",
    "\n",
    "# Adicionar grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Exibir gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 3 - Previs√£o por Ve√≠culo e Origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origem = df.groupby(['Veiculo', 'Ano', 'M√™s', 'Origem']).agg({\n",
    "    'IPCA ES': 'mean',                  \n",
    "    'IPCA BR': 'mean',                 \n",
    "    'Taxa Ac. TRI % PIB': 'mean',    \n",
    "    'Vl Liquido Final s/ normaliza√ß√£o': 'sum', \n",
    "    'UEN': 'first',                    \n",
    "    'pmc_indice_ES': 'mean',            \n",
    "    'pmc_indice_sazonal_ES': 'mean',    \n",
    "    'pmc_variacao_ES': 'mean',         \n",
    "    'pmc_indice_Brasil': 'mean',       \n",
    "    'pmc_indice_sazonal_Brasil': 'mean',\n",
    "    'pmc_variacao_Brasil': 'mean'       \n",
    "}).reset_index()\n",
    "\n",
    "# Exibir o DataFrame resultante\n",
    "df_origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origem['Vl Liquido Final normalizado'] = np.log1p(df_origem['Vl Liquido Final s/ normaliza√ß√£o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_origem = df_origem[['Veiculo','Ano','M√™s', 'Taxa Ac. TRI % PIB', 'Origem']] #df apenas com as features desejadas\n",
    "y_origem = df_origem['Vl Liquido Final normalizado'] #Coluna desejada para a previs√£o\n",
    "\n",
    "# Separando os dados em treino (60%), valida√ß√£o (20%) e teste (20%)\n",
    "X_train_origem, X_temp_origem, y_train_origem, y_temp_origem = train_test_split(x_origem, y_origem, test_size=0.4, random_state=42)\n",
    "\n",
    "# Dividindo o conjunto tempor√°rio (X_temp) em valida√ß√£o e teste\n",
    "X_val_origem, X_test_origem, y_val_origem, y_test_origem = train_test_split(X_temp_origem, y_temp_origem, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_origem = lgb.Dataset(X_train_origem, label=y_train_origem)\n",
    "val_data_origem = lgb.Dataset(X_val_origem, label=y_val_origem)\n",
    "\n",
    "params_origem = {'learning_rate': 0.06, \n",
    "          'num_leaves': 126, \n",
    "          'feature_fraction': 0.7, \n",
    "          'bagging_fraction': 0.9, \n",
    "          'bagging_freq': 10, \n",
    "          'n_estimators': 790,\n",
    "          'seed': 42}\n",
    "\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=100)]\n",
    "model_origem = lgb.train(params_origem, \n",
    "                  train_data_origem, \n",
    "                  num_boost_round=1000, \n",
    "                  valid_sets=[val_data_origem], \n",
    "                  callbacks=callbacks)\n",
    "\n",
    "\n",
    "y_pred_origem = model_origem.predict(X_test_origem, num_iteration=model_origem.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo das m√©tricas para o modelo\n",
    "rmse_origem = mean_squared_error(y_test_origem, y_pred_origem, squared=False)\n",
    "r2_origem = r2_score(y_test_origem, y_pred_origem)\n",
    "mae_origem = mean_absolute_error(y_test_origem, y_pred_origem)\n",
    "mape_origem = mean_absolute_percentage_error(y_test_origem, y_pred_origem)\n",
    "\n",
    "# Exibi√ß√£o das m√©tricas\n",
    "print(f'Teste - RMSE: {rmse_origem:.4f}')\n",
    "print(f'Teste - R¬≤: {r2_origem:.4f}')\n",
    "print(f'Teste - MAE: {mae_origem:.4f}')\n",
    "print(f'Teste - MAPE: {mape_origem:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo 3 com otimiza√ß√£o dos hiperpar√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignorar avisos\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# definir a fun√ß√£o objetivo para a otimiza√ß√£o\n",
    "def objective_origem_opt(trial):\n",
    "    params_origem_opt = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.07),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 130),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 0.8),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.7, 1),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1000),\n",
    "        'verbose': -1,  # Desativar logs desnecess√°rios\n",
    "        'seed': 42  \n",
    "    }\n",
    "    \n",
    "    train_data_origem_opt = lgb.Dataset(X_train_origem, label=y_train_origem)\n",
    "    val_data_origem_opt = lgb.Dataset(X_val_origem, label=y_val_origem)\n",
    "    \n",
    "    model_origem_opt = lgb.train(params_origem_opt, \n",
    "                      train_data_origem_opt, \n",
    "                      num_boost_round=1000, \n",
    "                      valid_sets=[val_data_origem_opt], \n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=100)])  # Sem log_evaluation\n",
    "    \n",
    "    y_pred_origem_opt = model_origem_opt.predict(X_val_origem, num_iteration=model_origem_opt.best_iteration)\n",
    "    \n",
    "    mape_origem_opt = mean_absolute_percentage_error(y_val_origem, y_pred_origem_opt) * 100\n",
    "    return mape_origem_opt\n",
    "\n",
    "# otimiza√ß√£o\n",
    "study_origem_opt = optuna.create_study(direction='minimize')  \n",
    "study_origem_opt.optimize(objective_origem_opt, n_trials=50)\n",
    "\n",
    "# mostrar melhores hiperpar√¢metros\n",
    "print(\"Melhores hiperpar√¢metros encontrados:\", study_origem_opt.best_params)\n",
    "\n",
    "# treinar o modelo com os melhores hiperpar√¢metros\n",
    "best_params_origem_opt = study_origem_opt.best_params\n",
    "best_params_origem_opt['verbose'] = -1  # Desativar logs desnecess√°rios\n",
    "\n",
    "train_data_origem_opt = lgb.Dataset(X_train_origem, label=y_train_origem)\n",
    "val_data_origem_opt = lgb.Dataset(X_val_origem, label=y_val_origem)\n",
    "\n",
    "best_model_origem_opt = lgb.train(best_params_origem_opt, \n",
    "                       train_data_origem_opt, \n",
    "                       num_boost_round=1000, \n",
    "                       valid_sets=[val_data_origem_opt], \n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "\n",
    "# avalia√ß√£o final no conjunto de teste\n",
    "y_pred_test_origem_opt = best_model_origem_opt.predict(X_test_origem, num_iteration=best_model_origem_opt.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo das m√©tricas para o modelo\n",
    "rmse_origem_opt = mean_squared_error(y_test_origem, y_pred_test_origem_opt, squared=False)\n",
    "r2_origem_opt = r2_score(y_test_origem, y_pred_test_origem_opt)\n",
    "mae_origem_opt = mean_absolute_error(y_test_origem, y_pred_test_origem_opt)\n",
    "mape_origem_opt = mean_absolute_percentage_error(y_test_origem, y_pred_test_origem_opt)\n",
    "\n",
    "# Exibi√ß√£o das m√©tricas\n",
    "print('M√©tricas com a Otimiza√ß√£o dos Hiperpar√¢metros')\n",
    "print(f'Teste - RMSE: {rmse_origem_opt:.4f}')\n",
    "print(f'Teste - R¬≤: {r2_origem_opt:.4f}')\n",
    "print(f'Teste - MAE: {mae_origem_opt:.4f}')\n",
    "print(f'Teste - MAPE: {mape_origem_opt:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar valores reais vs previstos para o modelo otimizado\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Garantir que os dados de teste estejam alinhados e sem reset de √≠ndices desnecess√°rios\n",
    "plt.plot(y_test_origem.reset_index(drop=True)[:100], label='Valores Reais', color='#6baed6', linestyle='-')\n",
    "plt.plot(y_pred_test_origem_opt[:100], label='Valores Previstos', color='red', linestyle='--')\n",
    "\n",
    "# Configura√ß√µes do gr√°fico\n",
    "plt.xlabel('√çndice')\n",
    "plt.ylabel('Valor ( ln(1+x) )')\n",
    "plt.title('An√°lise das Predi√ß√µes do Modelo 3: Valores Reais vs. Valores Previstos (Primeiros 100 Dados)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do gr√°fico\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Gr√°fico de dispers√£o dos valores reais vs previstos\n",
    "plt.scatter(y_test_origem, y_pred_test_origem_opt, alpha=0.6, edgecolors='k', color='blue', label='Valores reais em compara√ß√£o com os preditos')\n",
    "\n",
    "# Adicionar uma linha de refer√™ncia (linha ideal onde os valores reais s√£o iguais aos previstos)\n",
    "plt.plot([y_test_origem.min(), y_test_origem.max()], [y_test_origem.min(), y_test_origem.max()], 'r--', lw=2, label='Linha de Posi√ß√£o Ideal')\n",
    "\n",
    "# Adicionando a legenda\n",
    "plt.legend(title=\"Legenda\", loc=\"lower right\", fontsize = 10)  # Adiciona a legenda no canto inferior direito\n",
    "\n",
    "# R√≥tulos e t√≠tulo\n",
    "plt.xlabel('Valores Reais ( ln(1+x) )')\n",
    "plt.ylabel('Valores Preditos ( ln(1+x) )')\n",
    "plt.title('Diagrama de Dispers√£o das Predi√ß√µes do Modelo 3 (Ve√≠culo e Origem)')\n",
    "\n",
    "# Adicionar grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Exibir gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Sistema de Previs√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa se√ß√£o, ser√° poss√≠vel realizar as previs√µes de receita, utilizando o modelo LightGBM. Ser√° necess√°rio fornecer o ano, m√™s, estimativa da 'Taxa Ac. TRI % PIB', o ve√≠culo e, se desejado, a origem ou segmento espec√≠fico que se deseja prever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ‚ö†Ô∏è Antes de utilizar os sistemas de previs√£o execute a c√©lula que possui o sistema que deseja utilizar mais uma vez. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1 - Previs√£o por Ve√≠culo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicion√°rio com os nomes dos ve√≠culos\n",
    "veiculos_nomes = {\n",
    "    0: 'CBN VITORIA',\n",
    "    1: 'GAZETA FM LINHARES',\n",
    "    2: 'GAZETA FM VITORIA',\n",
    "    3: 'GAZETA PRODUCOES',\n",
    "    4: 'INTERNET',\n",
    "    5: 'LITORAL FM',\n",
    "    6: 'LITORAL FM NOROESTE',\n",
    "    7: 'LITORAL FM NORTE',\n",
    "    8: 'LITORAL FM SUL',\n",
    "    9: 'PORTAL G1/GE/HOME',\n",
    "    10: 'PRODUCAO',\n",
    "    11: 'RADIO MIX VITORIA',\n",
    "    12: 'TV GAZETA',\n",
    "    13: 'TV GAZETA NOROESTE',\n",
    "    14: 'TV GAZETA N0RTE',\n",
    "    15: 'TV GAZETA SUL',\n",
    "    16: 'TV NOROESTE PRODUCOES',\n",
    "    17: 'TV NORTE PRODUCOES',\n",
    "    18: 'TV SUL PRODUCOES'\n",
    "}\n",
    "\n",
    "# Estilo para aumentar o espa√ßo reservado para a descri√ß√£o\n",
    "style = {'description_width': '160px'}\n",
    "\n",
    "# Dropdown para selecionar o ve√≠culo\n",
    "veiculo_dropdown = widgets.Dropdown(\n",
    "    options=[(v, k) for k, v in veiculos_nomes.items()],\n",
    "    value=None, \n",
    "    description='Ve√≠culo:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Widgets para o ano e taxas trimestrais\n",
    "ano_widget_veiculo = widgets.IntText(value=None, description='Ano:')\n",
    "taxa_ac_q1_veiculo = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 1 (%):', style= style)\n",
    "taxa_ac_q2_veiculo = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 2 (%):',style= style)\n",
    "taxa_ac_q3_veiculo = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 3 (%):', style= style)\n",
    "taxa_ac_q4_veiculo = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 4 (%):', style= style)\n",
    "\n",
    "# Fun√ß√£o para realizar a previs√£o dos pr√≥ximos 12 meses com taxas trimestrais diferentes\n",
    "def fazer_previsao_veiculo(veiculo, ano, taxas):\n",
    "    # Distribuir as taxas trimestrais ao longo dos meses\n",
    "    meses = list(range(1, 13))\n",
    "    taxas_trimestrais = [taxas[0]] * 3 + [taxas[1]] * 3 + [taxas[2]] * 3 + [taxas[3]] * 3\n",
    "    \n",
    "    # Criar DataFrame para os 12 meses\n",
    "    data_para_previsao = pd.DataFrame({\n",
    "        'Veiculo': [veiculo] * 12,\n",
    "        'Ano': [ano] * 12,\n",
    "        'M√™s': meses,\n",
    "        'Taxa Ac. TRI % PIB': taxas_trimestrais\n",
    "    })\n",
    "    \n",
    "    # Fazer previs√£o com o modelo para os 12 meses\n",
    "    previsoes_veiculo = model_veiculo.predict(data_para_previsao)\n",
    "    previsoes_originais_veiculo = np.expm1(previsoes_veiculo)\n",
    "    \n",
    "    # Exibir previs√µes por m√™s\n",
    "    print(f'Previs√µes para {veiculos_nomes[veiculo]} em {ano}:')\n",
    "    for mes, previsao_original in zip(meses, previsoes_originais_veiculo):\n",
    "        print(f'M√™s {mes}: {previsao_original:.2f}')\n",
    "\n",
    "     # Gerar gr√°fico de linhas com as previs√µes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(meses, previsoes_originais_veiculo, marker='o', linestyle='-', color='b', label='Previs√£o')\n",
    "    plt.title(f'Previs√µes Mensais para {veiculos_nomes[veiculo]} - ({ano})')\n",
    "    plt.xlabel('M√™s')\n",
    "    plt.ylabel('Previs√£o de Receita (R$)')\n",
    "    plt.xticks(meses)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar resultados anteriores separadamente para cada m√™s\n",
    "    for mes in meses:\n",
    "        resultados_anteriores = df_veiculo[['M√™s', 'Ano', 'Vl Liquido Final s/ normaliza√ß√£o']].loc[\n",
    "            (df_veiculo['Veiculo'] == veiculo) & (df_veiculo['M√™s'] == mes)\n",
    "        ]\n",
    "        \n",
    "        print(f'\\nReceitas anteriores - M√™s {mes}:')\n",
    "        display(resultados_anteriores)\n",
    "\n",
    "# Bot√£o para acionar a previs√£o\n",
    "botao_previsao_veiculo = widgets.Button(description=\"Fazer Previs√£o\")\n",
    "\n",
    "# Fun√ß√£o do bot√£o\n",
    "def on_button_click(b):\n",
    "    taxas_veiculo = [taxa_ac_q1_veiculo.value, taxa_ac_q2_veiculo.value, taxa_ac_q3_veiculo.value, taxa_ac_q4_veiculo.value]\n",
    "    fazer_previsao_veiculo(veiculo_dropdown.value, ano_widget_veiculo.value, taxas_veiculo)\n",
    "\n",
    "botao_previsao_veiculo.on_click(on_button_click)\n",
    "\n",
    "# Exibir os widgets e o bot√£o\n",
    "display(veiculo_dropdown, ano_widget_veiculo, taxa_ac_q1_veiculo, taxa_ac_q2_veiculo, taxa_ac_q3_veiculo, taxa_ac_q4_veiculo, botao_previsao_veiculo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 2 - Previs√£o por Ve√≠culo e Segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicion√°rios com os nomes dos segmentos\n",
    "segmentos_nomes = {\n",
    "    0: 'AGROPECUARIA',\n",
    "    1: 'ALIMENTOS',\n",
    "    2: 'BEBIDAS',\n",
    "    3: 'BENS INDUSTRIAIS MAT. PRIMA',\n",
    "    4: 'BRINQUEDOS E DIVERSOES',\n",
    "    5: 'COMERCIO',\n",
    "    6: 'CONDOMINIOS',\n",
    "    7: 'EDUCACAO/MEIOS DE COMUNICACAO',\n",
    "    8: 'EQUIP/MAT/ESCRIT/LOJA/ESCOLA',\n",
    "    9: 'FOTO / OTICA / CINE / SOM',\n",
    "    10: 'INDUSTRIA DA CONSTRUCAO',\n",
    "    11: 'INTERNET',\n",
    "    12: 'LIMPEZA / HIGIENE DOMESTICA',\n",
    "    13: 'MEIOS DE TRANSPORTES E AFINS',\n",
    "    14: 'MERCADO FINANCEIRO',\n",
    "    15: 'MOVEIS E DECORACOES',\n",
    "    16: 'OUTRAS RECEITAS',\n",
    "    17: 'OUTROS',\n",
    "    18: 'PERFUMARIA / FARMACIA',\n",
    "    19: 'SERVICOS',\n",
    "    20: 'SERVICOS DE TELECOMUNICACOES',\n",
    "    21: 'TEXTIL / VESTUARIO',\n",
    "    22: 'UTILIDADES DOMESTICAS'\n",
    "}\n",
    "\n",
    "# Estilo para ajustar a largura da descri√ß√£o dos widgets\n",
    "style = {'description_width': '160px'}\n",
    "\n",
    "# Dropdowns para selecionar o ve√≠culo e o segmento\n",
    "veiculo_dropdown = widgets.Dropdown(\n",
    "    options=[(v, k) for k, v in veiculos_nomes.items()],\n",
    "    value=None,  # Default para 'TV GAZETA'\n",
    "    description='Ve√≠culo:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "segmento_dropdown = widgets.Dropdown(\n",
    "    options=[(v, k) for k, v in segmentos_nomes.items()],\n",
    "    value=None,  # Default para 'AGROPECUARIA'\n",
    "    description='Segmento:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "# Widgets para o ano e as taxas trimestrais\n",
    "ano_widget_segmento = widgets.IntText(value=None, description='Ano:', style=style)\n",
    "taxa_ac_q1_segmento = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 1 (%):', style=style)\n",
    "taxa_ac_q2_segmento = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 2 (%):', style=style)\n",
    "taxa_ac_q3_segmento = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 3 (%):', style=style)\n",
    "taxa_ac_q4_segmento = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 4 (%):', style=style)\n",
    "\n",
    "# Fun√ß√£o para realizar a previs√£o\n",
    "def fazer_previsao_segmento(veiculo, segmento, ano, taxas):\n",
    "    meses = list(range(1, 13))\n",
    "    taxas_trimestrais = [taxas[0]] * 3 + [taxas[1]] * 3 + [taxas[2]] * 3 + [taxas[3]] * 3\n",
    "    \n",
    "    # Criar DataFrame para os 12 meses\n",
    "    data_para_previsao = pd.DataFrame({\n",
    "        'Veiculo': [veiculo] * 12,\n",
    "        'Ano': [ano] * 12,\n",
    "        'M√™s': meses,\n",
    "        'Segmento': [segmento] * 12,\n",
    "        'Taxa Ac. TRI % PIB': taxas_trimestrais\n",
    "    })\n",
    "    \n",
    "    # Fazer previs√£o com o modelo para os 12 meses\n",
    "    previsoes_segmento = model_segmento.predict(data_para_previsao)\n",
    "    previsoes_originais_segmento = np.expm1(previsoes_segmento)\n",
    "    \n",
    "    # Exibir previs√µes por m√™s\n",
    "    print(f'Previs√µes para {veiculos_nomes[veiculo]} no segmento {segmentos_nomes[segmento]} em {ano}:')\n",
    "    for mes, previsao_original in zip(meses, previsoes_originais_segmento):\n",
    "        print(f'M√™s {mes}: {previsao_original:.2f}')\n",
    "\n",
    "         # Gerar gr√°fico de linhas com as previs√µes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(meses, previsoes_originais_segmento, marker='o', linestyle='-', color='b', label='Previs√£o')\n",
    "    plt.title(f'Previs√µes Mensais para {veiculos_nomes[veiculo]} - Segmento {segmentos_nomes[segmento]} ({ano})')\n",
    "    plt.xlabel('M√™s')\n",
    "    plt.ylabel('Previs√£o de Receita (R$)')\n",
    "    plt.xticks(meses)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar receitas anteriores para cada m√™s\n",
    "    for mes in meses:\n",
    "        resultados_anteriores = df_segmento[['M√™s', 'Ano', 'Vl Liquido Final s/ normaliza√ß√£o']].loc[\n",
    "            (df_segmento['Veiculo'] == veiculo) & \n",
    "            (df_segmento['M√™s'] == mes) & \n",
    "            (df_segmento['Segmento'] == segmento)\n",
    "        ]\n",
    "        \n",
    "        print(f'\\nReceitas anteriores - M√™s {mes}:')\n",
    "        display(resultados_anteriores)\n",
    "\n",
    "# Bot√£o para acionar a previs√£o\n",
    "botao_previsao_segmento = widgets.Button(description=\"Fazer Previs√£o\")\n",
    "\n",
    "# Fun√ß√£o do bot√£o\n",
    "def on_button_click(b):\n",
    "    taxas_segmento = [taxa_ac_q1_segmento.value, taxa_ac_q2_segmento.value, taxa_ac_q3_segmento.value, taxa_ac_q4_segmento.value]\n",
    "    fazer_previsao_segmento(veiculo_dropdown.value, segmento_dropdown.value, ano_widget_segmento.value, taxas_segmento)\n",
    "\n",
    "botao_previsao_segmento.on_click(on_button_click)\n",
    "\n",
    "# Exibir os widgets e o bot√£o\n",
    "display(veiculo_dropdown, segmento_dropdown, ano_widget_segmento, taxa_ac_q1_segmento, taxa_ac_q2_segmento, taxa_ac_q3_segmento, taxa_ac_q4_segmento, botao_previsao_segmento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 3 - Previs√£o por Ve√≠culo e Origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicion√°rios com os nomes das origens\n",
    "origens_nomes = {\n",
    "    0: 'CH - CONTATO - CACHOEIRO',\n",
    "    1: 'CO - CONTATO - COLATINA',\n",
    "    2: 'LI - CONTATO - LINHARES',\n",
    "    3: 'MP - M√çDIA PROGRAM√ÅTICA',\n",
    "    4: 'RN - MERCADO NACIONAL',\n",
    "    5: 'VT - CONTATO - VIT√ìRIA'\n",
    "}\n",
    "\n",
    "# Estilo para ajustar a largura da descri√ß√£o dos widgets\n",
    "style = {'description_width': '160px'}\n",
    "\n",
    "# Dropdowns para selecionar o ve√≠culo e o segmento\n",
    "veiculo_dropdown = widgets.Dropdown(\n",
    "    options=[(v, k) for k, v in veiculos_nomes.items()],\n",
    "    value=None, \n",
    "    description='Ve√≠culo:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "origem_dropdown = widgets.Dropdown(\n",
    "    options=[(v, k) for k, v in origens_nomes.items()],\n",
    "    value=None,  # Default \n",
    "    description='Origem:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "# Widgets para o ano e as taxas trimestrais\n",
    "ano_widget_origem = widgets.IntText(value=None, description='Ano:', style=style)\n",
    "taxa_ac_q1_origem = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 1 (%):', style=style)\n",
    "taxa_ac_q2_origem = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 2 (%):', style=style)\n",
    "taxa_ac_q3_origem = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 3 (%):', style=style)\n",
    "taxa_ac_q4_origem = widgets.FloatText(value=None, description='Taxa Ac. do PIB - TRI 4 (%):', style=style)\n",
    "\n",
    "# Fun√ß√£o para realizar a previs√£o\n",
    "def fazer_previsao_origem(veiculo, origem, ano, taxas):\n",
    "    meses = list(range(1, 13))\n",
    "    taxas_trimestrais = [taxas[0]] * 3 + [taxas[1]] * 3 + [taxas[2]] * 3 + [taxas[3]] * 3\n",
    "    \n",
    "    # Criar DataFrame para os 12 meses\n",
    "    data_para_previsao = pd.DataFrame({\n",
    "        'Veiculo': [veiculo] * 12,\n",
    "        'Ano': [ano] * 12,\n",
    "        'M√™s': meses,\n",
    "        'Origem': [origem] * 12,\n",
    "        'Taxa Ac. TRI % PIB': taxas_trimestrais\n",
    "    })\n",
    "    \n",
    "    # Fazer previs√£o com o modelo para os 12 meses\n",
    "    previsoes_origem = model_origem.predict(data_para_previsao)\n",
    "    previsoes_originais_origem = np.expm1(previsoes_origem)\n",
    "    \n",
    "    # Exibir previs√µes por m√™s\n",
    "    print(f'Previs√µes para {veiculos_nomes[veiculo]} na origem {origens_nomes[origem]} em {ano}:')\n",
    "    for mes, previsao_original in zip(meses, previsoes_originais_origem):\n",
    "        print(f'M√™s {mes}: {previsao_original:.2f}')\n",
    "\n",
    "     # Gerar gr√°fico de linhas com as previs√µes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(meses, previsoes_originais_origem, marker='o', linestyle='-', color='b', label='Previs√£o')\n",
    "    plt.title(f'Previs√µes Mensais para {veiculos_nomes[veiculo]} - Origem {origens_nomes[origem]} ({ano})')\n",
    "    plt.xlabel('M√™s')\n",
    "    plt.ylabel('Previs√£o de Receita (R$)')\n",
    "    plt.xticks(meses)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar receitas anteriores para cada m√™s\n",
    "    for mes in meses:\n",
    "        resultados_anteriores = df_origem[['M√™s', 'Ano', 'Vl Liquido Final s/ normaliza√ß√£o']].loc[\n",
    "            (df_origem['Veiculo'] == veiculo) & \n",
    "            (df_origem['M√™s'] == mes) & \n",
    "            (df_origem['Origem'] == origem)\n",
    "        ]\n",
    "        \n",
    "        print(f'\\nReceitas anteriores - M√™s {mes}:')\n",
    "        display(resultados_anteriores)\n",
    "\n",
    "# Bot√£o para acionar a previs√£o\n",
    "botao_previsao_origem = widgets.Button(description=\"Fazer Previs√£o\")\n",
    "\n",
    "# Fun√ß√£o do bot√£o\n",
    "def on_button_click(b):\n",
    "    taxas_origem = [taxa_ac_q1_origem.value, taxa_ac_q2_origem.value, taxa_ac_q3_origem.value, taxa_ac_q4_origem.value]\n",
    "    fazer_previsao_origem(veiculo_dropdown.value, origem_dropdown.value, ano_widget_origem.value, taxas_origem)\n",
    "\n",
    "botao_previsao_origem.on_click(on_button_click)\n",
    "\n",
    "# Exibir os widgets e o bot√£o\n",
    "display(veiculo_dropdown, origem_dropdown, ano_widget_origem, taxa_ac_q1_origem, taxa_ac_q2_origem, taxa_ac_q3_origem, taxa_ac_q4_origem, botao_previsao_origem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÉ Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTHON. Documenta√ß√£o Python. Wiki Python Brasil. Dispon√≠vel em: https://wiki.python.org.br/DocumentacaoPython. Acesso em: 11 set. 2024.\n",
    "\n",
    "MATPLOTLIB. matplotlib.pyplot API. Vers√£o 3.5.3. Dispon√≠vel em: https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html. Acesso em: 11 set. 2024.\n",
    "\n",
    "SCIPY. SciPy Documentation. Dispon√≠vel em: https://docs.scipy.org/doc/scipy/. Acesso em: 11 set. 2024.\n",
    "\n",
    "PANDAS. Pandas Documentation. Dispon√≠vel em: https://pandas.pydata.org/docs/index.html#. Acesso em: 11 set. 2024.\n",
    "\n",
    "NUMPY. NumPy Documentation. Dispon√≠vel em: https://numpy.org/devdocs/. Acesso em: 11 set. 2024.\n",
    "\n",
    "SCIKIT-LEARN. Scikit-learn Documentation. Vers√£o 0.21. Dispon√≠vel em: https://scikit-learn.org/0.21/documentation.html. Acesso em: 11 set. 2024.\n",
    "\n",
    "SCIPY. SciPy Documentation. Dispon√≠vel em: https://docs.scipy.org/doc/. Acesso em: 11 set. 2024.\n",
    "\n",
    "SEABORN. Seaborn Documentation. Dispon√≠vel em: https://seaborn.pydata.org/. Acesso em: 11 set. 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
